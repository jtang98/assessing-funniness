{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"CW-NLP-FINAL-RC.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"widgets":{"application/vnd.jupyter.widget-state+json":{"c79feb70451d4149adcb316bdea40d5e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_15b52452fd2247ed875f835417e0f3a7","IPY_MODEL_aa1cea8cbc484bd1aa3acdb5e50c492d"],"layout":"IPY_MODEL_d10ab4d0d1264104b96b65f9dfdf63a7"}},"15b52452fd2247ed875f835417e0f3a7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_8b95e769e29343ad8c9e41f9b18f872b","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e32657f20b0645af91dc6c8984581603","value":231508}},"aa1cea8cbc484bd1aa3acdb5e50c492d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_adef4d20afa84b11811a8e04fccd86a5","placeholder":"​","style":"IPY_MODEL_0927c68261ea4d6eab5e9c944843bb8c","value":" 232k/232k [00:00&lt;00:00, 3.75MB/s]"}},"d10ab4d0d1264104b96b65f9dfdf63a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b95e769e29343ad8c9e41f9b18f872b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e32657f20b0645af91dc6c8984581603":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"adef4d20afa84b11811a8e04fccd86a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0927c68261ea4d6eab5e9c944843bb8c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac7ffcd28287428fb5ec4ba5e8033ef0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0731254f538846a9bb6cea3c61bb75b0","IPY_MODEL_9579d4fdeb9c402ebed1dc910f0f3dd0"],"layout":"IPY_MODEL_a0471dc7c7b545599cccbc16ffe56c8a"}},"0731254f538846a9bb6cea3c61bb75b0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_f01f590b37d948ac80f4e66bc2df9c61","max":433,"min":0,"orientation":"horizontal","style":"IPY_MODEL_73415ffff2884c85b4d0ff8a9be0edf9","value":433}},"9579d4fdeb9c402ebed1dc910f0f3dd0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b269fb9e00a14ebdba2fbee13899c1ea","placeholder":"​","style":"IPY_MODEL_65fc402c6cd24c33b1a46b0d54a66a36","value":" 433/433 [01:48&lt;00:00, 3.99B/s]"}},"a0471dc7c7b545599cccbc16ffe56c8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f01f590b37d948ac80f4e66bc2df9c61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73415ffff2884c85b4d0ff8a9be0edf9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"b269fb9e00a14ebdba2fbee13899c1ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65fc402c6cd24c33b1a46b0d54a66a36":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fde43c77e82a47d1b34c84d8119339c6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_75880863fd9f49adbb45dcdfd7229c43","IPY_MODEL_c70bd1684de94e53bef2b06b8aa3b698"],"layout":"IPY_MODEL_cae50ab3ef1c4950a9c89e0d1e692343"}},"75880863fd9f49adbb45dcdfd7229c43":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_216900567c904412a0ef94c00bea05b6","max":440473133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e6fdbda3df5f4198914f4c8c9e6bb8e5","value":440473133}},"c70bd1684de94e53bef2b06b8aa3b698":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d9e5e204ccc140d5bf22b4ac00e481ed","placeholder":"​","style":"IPY_MODEL_802c8fb1a4564b269649443708912632","value":" 440M/440M [00:07&lt;00:00, 56.4MB/s]"}},"cae50ab3ef1c4950a9c89e0d1e692343":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"216900567c904412a0ef94c00bea05b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6fdbda3df5f4198914f4c8c9e6bb8e5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"d9e5e204ccc140d5bf22b4ac00e481ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"802c8fb1a4564b269649443708912632":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"poe1Kk2f_1SJ"},"source":["### Coursework coding instructions (please also see full coursework spec)\n","\n","Please choose if you want to do either Task 1 or Task 2. You should write your report about one task only.\n","\n","For the task you choose you will need to do two approaches:\n","  - Approach 1, which can use use pre-trained embeddings / models\n","  - Approach 2, which should not use any pre-trained embeddings or models\n","We should be able to run both approaches from the same colab file\n","\n","#### Running your code:\n","  - Your models should run automatically when running your colab file without further intervention\n","  - For each task you should automatically output the performance of both models\n","  - Your code should automatically download any libraries required\n","\n","#### Structure of your code:\n","  - You are expected to use the 'train', 'eval' and 'model_performance' functions, although you may edit these as required\n","  - Otherwise there are no restrictions on what you can do in your code\n","\n","#### Documentation:\n","  - You are expected to produce a .README file summarising how you have approached both tasks\n","\n","#### Reproducibility:\n","  - Your .README file should explain how to replicate the different experiments mentioned in your report\n","\n","Good luck! We are really looking forward to seeing your reports and your model code!"]},{"cell_type":"markdown","metadata":{"id":"h74vXwFPRQM6"},"source":["# README\n","\n","All the information is included in the notebook, which is commented. **Needs GPU to be executed**\n","\n","The notebook includes our work for the task 1, with the two approaches: with and without pre-trained embeddings.\n","\n","The first approach presents those models:\n","- FunConv: a hand-made simple convolutional+MLP architecture, which uses GloVe-100 embeddings.\n","- BiLSTM.\n","- BERT.\n","\n","The second approach is composed of a pipeline of TF-IDF (features), PCA (dimensionality reduction) and XGBoost (for regression).\n","\n","All the models perform and describe their own data pre-processing."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mI63j3IdWOM8","executionInfo":{"elapsed":4998,"status":"ok","timestamp":1614635637522,"user":{"displayName":"Joël Tang","photoUrl":"","userId":"16257048276826027552"},"user_tz":0},"outputId":"2817cf44-ee69-46ff-cafc-eaa6cf6297e3"},"source":["#from google.colab import drive\n","#drive.mount('/content/drive')\n","#path = '/content/drive/MyDrive/Imperial (TP et CW term 2)/NLP/Coursework/'\n","path = '/content/'\n","\n","!gdown --id 1UgrdjcHHZmAthjusQDAKoSqd37up-41f\n","!gdown --id 1rY6A0cN_cxAMK3aMHlTFWxhbcLFomvQL"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1UgrdjcHHZmAthjusQDAKoSqd37up-41f\n","To: /content/train.csv\n","100% 948k/948k [00:00<00:00, 14.9MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1rY6A0cN_cxAMK3aMHlTFWxhbcLFomvQL\n","To: /content/dev.csv\n","100% 219k/219k [00:00<00:00, 7.08MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ofyoskFv_1SQ","executionInfo":{"elapsed":545326,"status":"ok","timestamp":1614636177859,"user":{"displayName":"Joël Tang","photoUrl":"","userId":"16257048276826027552"},"user_tz":0},"outputId":"ce551357-bf8f-455f-efa7-9984072b0f61"},"source":["# You will need to download any word embeddings required for your code, e.g.:\n","\n","#%cd /content/drive/MyDrive/Imperial (TP et CW term 2)/NLP/Coursework/\n","!wget http://nlp.stanford.edu/data/glove.6B.zip\n","!unzip glove.6B.zip\n","\n","# For any packages that Colab does not provide auotmatically you will also need to install these below, e.g.:\n","\n","#! pip install torch"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-03-01 21:53:56--  http://nlp.stanford.edu/data/glove.6B.zip\n","Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n","--2021-03-01 21:53:56--  https://nlp.stanford.edu/data/glove.6B.zip\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n","--2021-03-01 21:53:56--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n","Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n","Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 862182613 (822M) [application/zip]\n","Saving to: ‘glove.6B.zip’\n","\n","glove.6B.zip        100%[===================>] 822.24M  1.92MB/s    in 8m 35s  \n","\n","2021-03-01 22:02:32 (1.60 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n","\n","Archive:  glove.6B.zip\n","  inflating: glove.6B.50d.txt        \n","  inflating: glove.6B.100d.txt       \n","  inflating: glove.6B.200d.txt       \n","  inflating: glove.6B.300d.txt       \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DwVEtBRmkXv8"},"source":["# Approach 1\n"]},{"cell_type":"markdown","metadata":{"id":"7HVuKulF2ini"},"source":["##FunConv : a simple CNN"]},{"cell_type":"markdown","metadata":{"id":"pwCqh30Q2uK6"},"source":["### Set up"]},{"cell_type":"code","metadata":{"id":"2pfDWCAZ_1SR"},"source":["# Imports\n","\n","import torch\n","import torch.nn as nn\n","import pandas as pd\n","import numpy as np\n","from sklearn.feature_extraction.text import CountVectorizer\n","from torch.utils.data import Dataset, random_split\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","import codecs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QhBzWYPE_1SR"},"source":["# Setting random seed and device\n","SEED = 1\n","\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yWWmHzWA_1ST"},"source":["# We define our training loop\n","def train(train_iter, dev_iter, model, number_epoch, loss_fn):\n","    \"\"\"\n","    Training loop for the model, which calls on eval to evaluate after each epoch\n","    \"\"\"\n","\n","    \n","    print(\"Training model.\")\n","\n","    for epoch in range(1, number_epoch+1):\n","\n","        model.train()\n","        epoch_loss = 0\n","        epoch_sse = 0\n","        no_observations = 0  # Observations used for training so far\n","\n","        for batch in train_iter:\n","\n","            feature, target = batch\n","\n","            feature, target = feature.to(device), target.to(device)\n","\n","            no_observations = no_observations + target.shape[0]\n","\n","            predictions = model(feature).squeeze(1)\n","\n","            optimizer.zero_grad()\n","\n","            loss = loss_fn(predictions, target)\n","            #print(f'The loss is {loss.item()}')\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            sse, __ = model_performance(predictions.detach().cpu().numpy(), target.detach().cpu().numpy())\n","\n","            epoch_loss += loss.item()*target.shape[0]\n","            epoch_sse += sse\n","\n","        valid_loss, valid_mse, __, __ = eval(dev_iter, model)\n","\n","        epoch_loss, epoch_mse = epoch_loss / no_observations, epoch_sse / no_observations\n","        print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.5f} | Train MSE: {epoch_mse:.2f} | Train RMSE: {epoch_mse**0.5:.2f} | \\\n","        Val. Loss: {valid_loss:.2f} | Val. MSE: {valid_mse:.2f} |  Val. RMSE: {valid_mse**0.5:.2f} |')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VJAfkJdB_1SU"},"source":["# We evaluate performance on our dev set\n","def eval(data_iter, model):\n","    \"\"\"\n","    Evaluating model performance on the dev set\n","    \"\"\"\n","    model.eval()\n","    epoch_loss = 0\n","    epoch_sse = 0\n","    pred_all = []\n","    trg_all = []\n","    no_observations = 0\n","\n","    with torch.no_grad():\n","        for batch in data_iter:\n","            feature, target = batch\n","\n","            feature, target = feature.to(device), target.to(device)\n","\n","            no_observations = no_observations + target.shape[0]\n","\n","            predictions = model(feature).squeeze(1)\n","            loss = loss_fn(predictions, target)\n","\n","            # We get the mse\n","            pred, trg = predictions.detach().cpu().numpy(), target.detach().cpu().numpy()\n","            sse, __ = model_performance(pred, trg)\n","\n","            epoch_loss += loss.item()*target.shape[0]\n","            epoch_sse += sse\n","            pred_all.extend(pred)\n","            trg_all.extend(trg)\n","\n","    return epoch_loss/no_observations, epoch_sse/no_observations, np.array(pred_all), np.array(trg_all)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tmr3rIq8_1SU"},"source":["# How we print the model performance\n","def model_performance(output, target, print_output=False):\n","    \"\"\"\n","    Returns SSE and MSE per batch (printing the MSE and the RMSE)\n","    \"\"\"\n","\n","    sq_error = (output - target)**2\n","\n","    sse = np.sum(sq_error)\n","    mse = np.mean(sq_error)\n","    rmse = np.sqrt(mse)\n","\n","    if print_output:\n","        print(f'| MSE: {mse:.2f} | RMSE: {rmse:.2f} |')\n","\n","    return sse, mse"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6As2uPfv2yTq"},"source":["### Pre-processing\n","\n","We pre-process our data, by tokenizing it with respect to spaces. We also remove left and right leading spaces, and we apply lowercasing to all the sentences."]},{"cell_type":"code","metadata":{"id":"8jmG1e2n_1SV"},"source":["import re\n","import string\n","\n","def clean_tok(inp):\n","    sentence_wo_lspaces = inp.strip().lower() #remove left and right leading spaces + lowercase\n","    clean_wo_lspaces_numbers = sentence_wo_lspaces #remove numbers is useless, it will not appear in pre-trained embeddings\n","    clean_sentence = clean_wo_lspaces_numbers\n","\n","    clean_sentence_tokenized = []\n","    for token in clean_sentence.split(\" \"):\n","        clean_sentence_tokenized.append(token)\n","\n","    return clean_sentence_tokenized\n","\n","def create_vocab(data):\n","    \"\"\"\n","    Creating a corpus of all the tokens used\n","    \"\"\"\n","    tokenized_corpus = [] # Let us put the tokenized corpus in a list\n","    tokenized_replaced = []\n","    tokenized_edits = []\n","\n","    for row in data:\n","\n","        if data.ndim > 1:\n","            #clean the data (preprocessing) before tokenization\n","            tokenized_corpus.append(clean_tok(row[0]))\n","            tokenized_replaced.append(clean_tok(row[1]))\n","            tokenized_edits.append(clean_tok(row[2]))\n","\n","        else:\n","            tokenized_corpus.append(clean_tok(row))\n","\n","    # Create single list of all vocabulary\n","    vocabulary = []  # Let us put all the tokens (mostly words) appearing in the vocabulary in a list\n","\n","    for sentence in tokenized_corpus:\n","        for token in sentence:\n","            if token not in vocabulary:\n","                if True:\n","                    vocabulary.append(token)\n","    if data.ndim > 1:\n","        for sentence in tokenized_replaced:\n","            for token in sentence:\n","                if token not in vocabulary:\n","                    if True:\n","                        vocabulary.append(token)\n","        for sentence in tokenized_edits:\n","            for token in sentence:\n","                if token not in vocabulary:\n","                    if True:\n","                        vocabulary.append(token)\n","\n","    return vocabulary, tokenized_corpus, tokenized_replaced, tokenized_edits"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k6Ha1SEGEpHp"},"source":["The padding is operated in the collate_fn_padd() function: we pad with 0 the sentences (which are the original headlines)."]},{"cell_type":"code","metadata":{"id":"sgAPS5Yh_1SV"},"source":["def collate_fn_padd(batch):\n","    '''\n","    Padds batch of variable length\n","\n","    note: it converts things ToTensor manually here since the ToTensor transform\n","    assume it takes in images rather than arbitrary tensors.\n","    '''\n","    batch_labels = [l for f, l in batch]\n","    batch_features = [f for f, l in batch]\n","\n","    ## get sequence lengths\n","    lengths = torch.tensor([ t.shape[0] for t in batch_features ])\n","\n","    ## pad\n","    batch_features = [ torch.LongTensor(t) for t in batch_features ]\n","    batch_features = torch.nn.utils.rnn.pad_sequence(batch_features, padding_value=0, batch_first=True)\n","    return torch.LongTensor(batch_features), torch.FloatTensor(batch_labels)\n","\n","class Task1Dataset(Dataset):\n","\n","    def __init__(self, train_data, labels):\n","        self.x_train = train_data\n","        self.y_train = labels\n","\n","    def __len__(self):\n","        return len(self.y_train)\n","\n","    def __getitem__(self, item):\n","        return self.x_train[item], self.y_train[item]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PF3cBbsk3BXg"},"source":["### Model\n","\n","The model is a simple convolutional neural network, taking as an input the encoded sentences (headlines), the modified word and the modifying word. Then, it applies the pre-trained embedding (here, GloVe embeddings of size 100) and a convolutional layer. Then, FC layers serve the regression task."]},{"cell_type":"code","metadata":{"id":"3-ePF1-xA-g9"},"source":["import torch.nn.functional as F\n","\n","class FunConv(nn.Module): # A simple convolutional model\n","\n","    def __init__(self, embedding_dim, vocab_size, device):\n","\n","        super(FunConv, self).__init__()\n","        self.train()\n","\n","        self.device = device\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","\n","        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3, 1))\n","\n","        self.fc1 = nn.Linear(in_features=8*embedding_dim, out_features=16)\n","        self.fc2 = nn.Linear(16, 1)\n","\n","\n","    def forward(self, sentence):\n","\n","        batch_size = sentence.shape[0]\n","        sentence = sentence.unsqueeze(1) #(B, 1, N)\n","        embedded = self.embedding(sentence) #(B, 1, N, EMB_DIM)\n","\n","        #embedded of shape (B, N, EMB_DIM)\n","\n","        mask = (embedded[:,:,2:,:] != 0)*1.0\n","        embedded[:,:,2] = torch.sum(embedded[:,:,2:,:], dim=2) / torch.sum(mask, dim=2)\n","        embedded = embedded[:,:,0:3,:] #remove all the rest of the cols\n","\n","        out = F.elu(self.conv1(embedded))\n","\n","        out = torch.flatten(out, start_dim=1)\n","\n","        out = F.elu(self.fc1(out))\n","        out = 3.0*torch.sigmoid(self.fc2(out))\n","        #out = F.relu(self.fc2(out))\n","\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7f16Hgso3El9"},"source":["### Train, metrics, performances"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J6-bu3Yq_1SW","scrolled":true,"executionInfo":{"elapsed":695967,"status":"ok","timestamp":1614636328523,"user":{"displayName":"Joël Tang","photoUrl":"","userId":"16257048276826027552"},"user_tz":0},"outputId":"b3d27a09-be1b-4e54-d928-922a9b31727a"},"source":["## Approach 1 code, using functions defined above:\n","\n","# Number of epochs\n","epochs = 10\n","# Proportion of training data for train compared to dev\n","train_proportion = 0.80\n","\n","# Load data\n","train_df = pd.read_csv(path + 'train.csv')\n","test_df = pd.read_csv(path + 'dev.csv')\n","\n","# To work on small subset of the dataset\n","#train_df = train_df.head(50)\n","#test_df = test_df.head(50)\n","\n","# Parse the relevant features\n","replaced_words = []\n","sentences = []\n","for s in train_df['original'].tolist():\n","    start = s.find('<') + 1\n","    end = s.find('/>', start)\n","    replaced_words.append(s[start:end])\n","    s = s.replace('<','')\n","    s = s.replace('/>','')  \n","    sentences.append(s)\n","train_df['replaced_word'] = replaced_words\n","train_df['original'] = sentences\n","\n","replaced_words = []\n","sentences = []\n","for s in test_df['original'].tolist():\n","    start = s.find('<') + 1\n","    end = s.find('/>', start)\n","    replaced_words.append(s[start:end])\n","    s = s.replace('<','')\n","    s = s.replace('/>','')  \n","    sentences.append(s)\n","test_df['replaced_word'] = replaced_words\n","test_df['original'] = sentences\n","\n","\n","# We set our training data and test data\n","training_data = np.array(train_df[['original', 'replaced_word', 'edit']])\n","test_data = np.array(test_df[['original', 'replaced_word', 'edit']])\n","\n","# Creating word vectors\n","training_vocab, training_tokenized_corpus, training_tokenized_replaced, training_tokenized_edits = create_vocab(training_data)\n","test_vocab, test_tokenized_corpus, test_tokenized_replaced, test_tokenized_edits = create_vocab(test_data)\n","\n","# Creating joint vocab from test and train:\n","joint_vocab,_ ,_, _ = create_vocab(np.concatenate([training_data.flatten(), test_data.flatten()]))\n","\n","print(\"Vocab created.\")\n","\n","# We create representations for our tokens\n","wvecs = [] # word vectors\n","word2idx = [] # word2index\n","idx2word = []\n","\n","missing_word_stats = {}\n","\n","# This is a large file, it will take a while to load in the memory!\n","with codecs.open('glove.6B.100d.txt', 'r','utf-8') as f:\n","  index = 1 #0 is for padding\n","  for line in f.readlines():\n","    # Ignore the first line - first line typically contains vocab, dimensionality\n","    if len(line.strip().split()) > 3:\n","      word = line.strip().split()[0]\n","      if word in joint_vocab:\n","          (word, vec) = (word,\n","                     list(map(float,line.strip().split()[1:])))\n","          wvecs.append(vec)\n","          word2idx.append((word, index))\n","          idx2word.append((index, word))\n","          index += 1\n","\n","print(np.array(wvecs).shape)\n","\n","#wvecs = np.concatenate((np.zeros((1,100)), np.array(wvecs), np.random.randn(2,100)), axis=0) #inserted new words\n","wvecs = np.concatenate((np.zeros((1,100)), np.array(wvecs)), axis=0)\n","word2idx = dict(word2idx)\n","word2idx['<pad>'] = 0\n","#word2idx['brexit'] = index\n","#word2idx['daca'] = index+1\n","idx2word = dict(idx2word)\n","idx2word[0] = '<pad>'\n","#idx2word[index] = \"brexit\"\n","#idx2word[index+1] = \"daca\"\n","\n","\n","print(\"Embeddings loaded.\")\n","\n","# Populate a list of unknown words\n","for i in range(len(training_tokenized_corpus)):\n","    for k in training_tokenized_replaced[i]:\n","        if k not in word2idx:\n","            missing_word_stats[k] = missing_word_stats.get(k, 0) + 1\n","    for k in training_tokenized_edits[i]:\n","        if k not in word2idx:\n","            missing_word_stats[k] = missing_word_stats.get(k, 0) + 1\n","    for k in training_tokenized_corpus[i]:\n","        if k not in word2idx:\n","            missing_word_stats[k] = missing_word_stats.get(k, 0) + 1\n","\n","vectorized_seqs = []\n","for i in range(len(training_tokenized_corpus)):\n","    vectorized_seqs.append([word2idx[tok] if tok in word2idx else 0 for tok in training_tokenized_replaced[i]] + [word2idx[tok] if tok in word2idx else 0 for tok in training_tokenized_edits[i]] + [word2idx[tok] if tok in word2idx else 0 for tok in training_tokenized_corpus[i]])\n","\n","# To avoid any sentences being empty (if no words match to our word embeddings)\n","def clean_vec(vectorized_seqs, labels):\n","    y = []\n","    cleaned = []\n","    for i, x in enumerate(vectorized_seqs):\n","        if len(x) == 0 or x[0] == 0 or x[1] == 0 or sum(x[2:]) == 0:\n","            continue\n","        cleaned.append(np.array(x))\n","        y.append(labels[i])\n","    y = np.array(y)\n","    assert len(cleaned) == y.shape[0]\n","    return cleaned, y\n","\n","vectorized_seqs, mean_grades = clean_vec(vectorized_seqs, train_df['meanGrade'])\n","\n","INPUT_DIM = len(word2idx)\n","EMBEDDING_DIM = 100\n","BATCH_SIZE = 32\n","\n","#Inputs vectorized.\n","\n","model = FunConv(EMBEDDING_DIM, INPUT_DIM, device)\n","print(\"Model initialised.\")\n","\n","model.to(device)\n","\n","# We provide the model with our embeddings\n","model.embedding.weight.data.copy_(torch.from_numpy(wvecs))\n","\n","feature = vectorized_seqs\n","\n","# 'feature' is a list of lists, each containing embedding IDs for word tokens : CONVERTED TO NP.ARRAY\n","train_and_dev = Task1Dataset(feature, mean_grades)\n","\n","train_examples = round(len(train_and_dev)*train_proportion)\n","dev_examples = len(train_and_dev) - train_examples\n","\n","train_dataset, dev_dataset = random_split(train_and_dev,\n","                                           (train_examples,\n","                                            dev_examples))\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\n","dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\n","\n","print(\"Dataloaders created.\")\n","\n","loss_fn = nn.MSELoss()\n","loss_fn = loss_fn.to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=10e-5)\n","\n","train(train_loader, dev_loader, model, epochs, loss_fn)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Vocab created.\n","(11803, 100)\n","Embeddings loaded.\n","Model initialised.\n","Dataloaders created.\n","Training model.\n","| Epoch: 01 | Train Loss: 0.34549 | Train MSE: 0.35 | Train RMSE: 0.59 |         Val. Loss: 0.32 | Val. MSE: 0.32 |  Val. RMSE: 0.57 |\n","| Epoch: 02 | Train Loss: 0.31746 | Train MSE: 0.32 | Train RMSE: 0.56 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.56 |\n","| Epoch: 03 | Train Loss: 0.30805 | Train MSE: 0.31 | Train RMSE: 0.56 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.56 |\n","| Epoch: 04 | Train Loss: 0.30152 | Train MSE: 0.30 | Train RMSE: 0.55 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.56 |\n","| Epoch: 05 | Train Loss: 0.29568 | Train MSE: 0.30 | Train RMSE: 0.54 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.56 |\n","| Epoch: 06 | Train Loss: 0.28968 | Train MSE: 0.29 | Train RMSE: 0.54 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.55 |\n","| Epoch: 07 | Train Loss: 0.28328 | Train MSE: 0.28 | Train RMSE: 0.53 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.56 |\n","| Epoch: 08 | Train Loss: 0.27708 | Train MSE: 0.28 | Train RMSE: 0.53 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.55 |\n","| Epoch: 09 | Train Loss: 0.27030 | Train MSE: 0.27 | Train RMSE: 0.52 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.56 |\n","| Epoch: 10 | Train Loss: 0.26473 | Train MSE: 0.26 | Train RMSE: 0.51 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.56 |\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_w2BYfOK8UTT","executionInfo":{"elapsed":695964,"status":"ok","timestamp":1614636328525,"user":{"displayName":"Joël Tang","photoUrl":"","userId":"16257048276826027552"},"user_tz":0},"outputId":"c0234145-9471-452a-e60c-8b8872b09348"},"source":["print(\"The unknown words:\")\n","print(dict(sorted(missing_word_stats.items(), key=lambda item: item[1], reverse=True)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The unknown words:\n","{'brexit': 47, 'daca': 39, 'anti-trump': 28, 'trump-russia': 26, 'scaramucci': 18, 'trumpcare': 18, 'reince': 11, 'alt-right': 11, 'trump-era': 9, 'pro-trump': 9, '#metoo': 9, 'alt-left': 9, 'puzder': 9, 'at&amp;t': 8, 'co-opts': 8, 'gianforte': 7, 'shithole': 6, 'kushners': 6, 'bush-era': 6, 'wapo': 6, 'cnnpolitics.com': 6, 'selfie': 5, 'greitens': 5, 'selfies': 5, 'cryptocurrency': 5, 'shipworm': 5, 'dotards': 4, 'dotard': 4, 'chibok': 3, 'harder-line': 3, 'affirmative-action': 3, 'assault-style': 3, 'process-related': 3, 'picture-perfect': 3, 'schlapp': 3, 'medicare-for-all': 3, 'manassian': 3, 'satellite-carrying': 3, 'deplorableness': 3, 'tipline': 3, 'lie-detector': 3, 'ukraines': 3, 'russia-us': 3, 'party-switch': 3, 'kasich-hickenlooper': 3, 'street-friendly': 3, 'delingpole': 3, 'pocohontas': 3, 'trump-palin-nugent-kid': 3, 'aecon': 3, '300mw': 3, 'ex-pm': 3, '#war': 3, 'mega-colonies': 3, 'mcenany': 3, 'q&amp;a': 3, 'subreddit': 3, 'cyberweapons': 3, 'warmbier': 3, 'ex-fbi': 3, 'guantรกnamo': 3, 'trump-themed': 3, 'infowars.com': 3, 'three-mile-long': 3, 'myeshia': 3, 'cyber-related': 3, 'trump-duterte': 3, 'npr/ipsos': 3, 'ex-trump': 3, 'dubke': 3, 'sh*t': 3, 'huffpo': 3, 'maute': 3, 'ex-ag': 3, '!!!!!!!!!!': 3, 'pseudo-president': 3, 'anti-tillerson': 3, 'kusher': 3, 'jae-yong': 3, 'ex-sheriff': 3, '#womensmarch': 3, 'vision-mexico': 3, 'bigly': 3, 'flip-flopped': 3, 'job-starved': 3, 'rodchenkov': 3, 'disinvites': 3, 'pro-doug': 3, 'rebel-planted': 3, 'covfefe': 3, 'soros-funded': 3, 'russian-linked': 3, 'schlapps': 3, 'fact-checks': 3, 'frexit': 3, 'whcd': 3, 'under-the-radar': 3, 'renewable-energy': 3, 'russian-bought': 3, 'wannacry': 3, 'unfollowed': 3, 'twitter-obsessed': 3, 'nobost': 3, 'zarrab': 3, 'non-wealthy': 3, 'flip-flopping': 3, 'under-reports': 3, 'fact-check': 3, 'trump-like': 3, 'radiation-sniffing': 3, 'anti-globalist': 3, 'us-russia': 3, '418m': 3, 'pro-putin': 3, 'cash-free': 3, 'gaza-israel': 3, 'scud-class': 3, 'frightbart': 3, 'bridgegate': 3, 'exerpts': 3, 'counterspies': 3, '133m': 3, 'russia-trump': 3, 'famine-hit': 3, 'health-insurance': 3, 'republican-led': 3, 'anti-overdose': 3, 'neo-confederate': 3, 'pro-russia': 3, 'strzok': 3, 'russia-related': 3, 'us-russian': 3, 'trump-drawn': 3, 'inaguration': 3, 'stupid-ass': 3, 'gold-star': 3, 'anti-lgbtq': 3, 'coffee-drinking': 3, 'bad-mouth': 3, 'adulting': 3, 'gunmam': 3, 'weaponizes': 3, 'unplumbed': 3, '9,103': 3, 'election-meddling': 3, 'hack-vulnerable': 3, 'troway': 3, 'poo-poo': 3, 'over-promised': 3, 'putin-linked': 3, 'manafort-gates': 3, 'cave-dwellers': 3, '24,473': 3, 'voterbase': 3, 'tweetstorm': 3, 'onpolitics': 3, 'us-bound': 3, 'ca-25': 3, 'not-so-big': 3, 'anbang': 3, '85bn': 3, 'iranian-made': 3, 'anti-russia': 3, 'stormy-mueller-cohen': 3, 'fagggots': 3, 'nigggers': 3, 'self-branding': 3, 'body-slam': 3, 'regeni': 3, 'ex-federal': 3, 'house-approved': 3, 'small-donor': 3, 'cyber-hacking': 3, 'pre-obama': 3, 'retweeted': 3, 'mirzakhani': 3, 'ex-prosecutor': 3, 'shulkin': 3, 'over-performing': 3, 'trump-st': 3, 'trumpist': 3, 'hjiab': 3, 'russia-linked': 3, 'four-session': 3, '#nomorenazi': 3, 'pornhub': 3, 'company-funded': 3, 'alternative-facts': 3, '1928-2017': 3, 'ex-house': 3, 'doo-doo': 3, 'campaign-report': 3, 'ex-intelligence': 3, 'kuaishou': 3, 'tencent-led': 3, 'dashcam': 3, 'philando': 3, 'ex-aide': 3, 'nonbinary': 3, 'ex-rising': 3, 'destroye': 3, 'ex-felons': 3, 'choirul': 3, 'trumpenomics': 3, 'pro-assad': 3, 'westworld-style': 3, 'crownprince': 3, 'ex-son-in-law': 3, 'middle-of-the-night': 3, 'pussyhats': 3, 'die-in': 3, 'stand-your-ground': 3, 'trump-related': 3, '@realdonaldtrump': 3, 'creditloan': 3, 'pizzagate': 3, '2-party': 3, 'sychologists': 3, 'cyberwars': 3, 'hb2': 3, 'trump-haters': 3, 'shkreli': 3, 'mypillow': 3, 'countermemo': 3, 'child-welfare': 3, 'trumpism': 3, '#x27': 3, 'climate-denying': 3, 'tweetstorms': 3, '@cnn': 3, 'election-pollsters': 3, 'dnc-ukraine': 3, 'isis-controlled': 3, 'nevertrump': 3, 'uninsuredrepublican': 3, 'second-deadliest': 3, 'infowars': 3, 'full-throttle': 3, 'ex-clinton': 3, 'neo-mccarthyite': 3, 'ex-obama': 3, 'terror-affiliated': 3, 'scandel': 3, 'brexiteers': 3, 'binomo': 3, 'disinviting': 3, 'pro-kremlin': 3, 'libya-bound': 3, 'nra-approved': 3, 'gabbanelli': 3, 'cantabella': 3, 'ex-serbian': 2, 'hollyweed': 2, 'white-nationalist': 2, 'bamboozles': 2, 'ex-doctor': 2, '230k': 2, 'de-legitimize': 2, 'f*ck': 2, 'test-fires': 2, 'pro-growth': 2, 'pro-environment': 2, 'antifa': 2, 'oil-backed': 2, 'cross-subsidies': 2, 'puigdemont': 2, 'impanels': 2, 'once-sleepy': 2, 'anti-minority': 2, 'kompromat': 2, 'skill-building': 2, 'stupefy': 1, 'jives': 1, 'orangeness': 1, 'canoodles': 1, 'graveling': 1, 'catfishing': 1, 'redecorates': 1, 'bagpiping': 1, 'transtemporalize': 1, 'dumbs': 1, 'halloweeners': 1, 'ventilates': 1, 'emojis': 1, 'supersoaker': 1, 'myopics': 1, 'defenestrates': 1, 'catawampus': 1, 'coracobrachialis': 1, 'haemorrhoid': 1, 'misanthropics': 1, 'vellicate': 1, 'birdlime': 1, 'dipsomaniac': 1, 'psychoneurotic': 1, 'grovels': 1, 'pigsties': 1, 'hoedowns': 1, 'fakiness': 1, 'twerking': 1, 'dustpans': 1, 'piroshki': 1, 'dematerialize': 1, 'eructations': 1, 'crochets': 1, 'agoraphobics': 1, 'intoxicates': 1, 'rollerskater': 1, 'cornholing': 1, 'scrubdown': 1, 'popcorns': 1, 'greasiness': 1, 'primps': 1, 'lazies': 1, 'crackhouse': 1, 'horoscopists': 1, 'misandrist': 1, 'fruitworm': 1, 'kennelmaster': 1, 'pussyfoots': 1, 'paperclips': 1, 'kremlins': 1, 'braincells': 1, 'oldness': 1, 'vomitorium': 1, 'oglings': 1}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QrWqRqLAzZwy","executionInfo":{"elapsed":695960,"status":"ok","timestamp":1614636328526,"user":{"displayName":"Joël Tang","photoUrl":"","userId":"16257048276826027552"},"user_tz":0},"outputId":"c6b59bc2-8fcb-40c8-8b7c-a655fe94b695"},"source":["nz = 0\n","z = 0\n","for sentence in vectorized_seqs:\n","    for code in sentence:\n","        if code == 0:\n","            z += 1\n","        nz += 1\n","print(\"The proportion of unknown words\")\n","print(z/nz) # The proportion of unknown words"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The proportion of unknown words\n","0.0072776162153472945\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":140},"id":"vm3QCXoBsrcS","executionInfo":{"elapsed":695960,"status":"ok","timestamp":1614636328527,"user":{"displayName":"Joël Tang","photoUrl":"","userId":"16257048276826027552"},"user_tz":0},"outputId":"a94e2eb7-d7f6-4e74-c831-5d6292a49f47"},"source":["# This cell is for the inference on test set\n","\n","'''vectorized_seqs_infer = []\n","for i in range(len(test_tokenized_corpus)):\n","    vectorized_seqs_infer.append([word2idx[tok] if tok in word2idx else 0 for tok in test_tokenized_replaced[i]] + [word2idx[tok] if tok in word2idx else 0 for tok in test_tokenized_edits[i]] + [word2idx[tok] if tok in word2idx else 0 for tok in test_tokenized_corpus[i]])\n","\n","# To avoid any sentences being empty (if no words match to our word embeddings)\n","def clean_vec_infer(vectorized_seqs, ids):\n","    cleaned = []\n","    ids_out = []\n","    ids_not_selected = []\n","    for i, x in enumerate(vectorized_seqs):\n","        if len(x) == 0 or x[0] == 0 or x[1] == 0 or sum(x[2:]) == 0:\n","            ids_not_selected.append(ids[i])\n","            continue\n","        cleaned.append(np.array(x))\n","        ids_out.append(ids[i])\n","    return cleaned, ids_out, ids_not_selected\n","\n","\n","vectorized_seqs_infer, ids_out, ids_not_selected = clean_vec_infer(vectorized_seqs_infer, test_df['id'])\n","infer_dataset = Task1Dataset(vectorized_seqs_infer, np.zeros(len(vectorized_seqs_infer)))\n","infer_loader = torch.utils.data.DataLoader(infer_dataset, batch_size=1, collate_fn=collate_fn_padd)\n","\n","model.eval()\n","predictions = []\n","\n","for ele,_ in list(infer_loader):\n","    ele = ele.to(device)\n","    predictions.append(model(ele)[0, 0].item())\n","\n","df_id = pd.DataFrame({'id': ids_out})\n","df_pred = pd.DataFrame({'pred': predictions})\n","\n","res = pd.concat([df_id, df_pred], axis=1)\n","\n","## Add baseline for lines that have been deleted during cleaning 0.9355712114932938\n","\n","for id in ids_not_selected:\n","    res = res.append({\"id\": id, \"pred\": 0.9355712114932938}, ignore_index=True)\n","\n","res['id'] = res['id'].astype(int)'''\n","\n","#res.to_csv(path + 'out.csv', index=False, sep=',')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'vectorized_seqs_infer = []\\nfor i in range(len(test_tokenized_corpus)):\\n    vectorized_seqs_infer.append([word2idx[tok] if tok in word2idx else 0 for tok in test_tokenized_replaced[i]] + [word2idx[tok] if tok in word2idx else 0 for tok in test_tokenized_edits[i]] + [word2idx[tok] if tok in word2idx else 0 for tok in test_tokenized_corpus[i]])\\n\\n# To avoid any sentences being empty (if no words match to our word embeddings)\\ndef clean_vec_infer(vectorized_seqs, ids):\\n    cleaned = []\\n    ids_out = []\\n    ids_not_selected = []\\n    for i, x in enumerate(vectorized_seqs):\\n        if len(x) == 0 or x[0] == 0 or x[1] == 0 or sum(x[2:]) == 0:\\n            ids_not_selected.append(ids[i])\\n            continue\\n        cleaned.append(np.array(x))\\n        ids_out.append(ids[i])\\n    return cleaned, ids_out, ids_not_selected\\n\\n\\nvectorized_seqs_infer, ids_out, ids_not_selected = clean_vec_infer(vectorized_seqs_infer, test_df[\\'id\\'])\\ninfer_dataset = Task1Dataset(vectorized_seqs_infer, np.zeros(len(vectorized_seqs_infer)))\\ninfer_loader = torch.utils.data.DataLoader(infer_dataset, batch_size=1, collate_fn=collate_fn_padd)\\n\\nmodel.eval()\\npredictions = []\\n\\nfor ele,_ in list(infer_loader):\\n    ele = ele.to(device)\\n    predictions.append(model(ele)[0, 0].item())\\n\\ndf_id = pd.DataFrame({\\'id\\': ids_out})\\ndf_pred = pd.DataFrame({\\'pred\\': predictions})\\n\\nres = pd.concat([df_id, df_pred], axis=1)\\n\\n## Add baseline for lines that have been deleted during cleaning 0.9355712114932938\\n\\nfor id in ids_not_selected:\\n    res = res.append({\"id\": id, \"pred\": 0.9355712114932938}, ignore_index=True)\\n\\nres[\\'id\\'] = res[\\'id\\'].astype(int)'"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"GAtYNkC8ilw-"},"source":["##BiLSTM"]},{"cell_type":"code","metadata":{"id":"BMaf8_FxikZ8"},"source":["train_df = pd.read_csv(path + 'train.csv')\r\n","test_df = pd.read_csv(path + 'dev.csv')\r\n","words = []\r\n","sentences = []\r\n","edited_sentences = []\r\n","concat = []\r\n","fromto = []\r\n","i = 0\r\n","for s in train_df['original'].tolist():\r\n","  edit = train_df['edit'].tolist()[i]\r\n","  i += 1\r\n","  start = s.find('<') + 1\r\n","  end = s.find('/>', start)\r\n","  word = s[start:end]\r\n","  words.append(word)\r\n","  s = s.replace('<','')\r\n","  s = s.replace('/>','')  \r\n","  edit_s = s.replace(word,edit)\r\n","  sentences.append(s)\r\n","  edited_sentences.append(edit_s)\r\n","  concat.append(s + '. ' + edit_s)\r\n","  fromto.append('From ' + word + ' to ' + edit)\r\n","\r\n","\r\n","train_df['original_word'] = words\r\n","train_df['original'] = sentences\r\n","train_df['edit_sentence'] = edited_sentences\r\n","train_df['concat'] = concat\r\n","train_df['FromTo'] = fromto\r\n","\r\n","\r\n","words = []\r\n","sentences = []\r\n","edited_sentences = []\r\n","concat = []\r\n","fromto = []\r\n","i = 0\r\n","for s in test_df['original'].tolist():\r\n","  edit = test_df['edit'].tolist()[i]\r\n","  i += 1\r\n","  start = s.find('<') + 1\r\n","  end = s.find('/>', start)\r\n","  word = s[start:end]\r\n","  words.append(word)\r\n","  s = s.replace('<','')\r\n","  s = s.replace('/>','')  \r\n","  edit_s = s.replace(word,edit)\r\n","  sentences.append(s)\r\n","  edited_sentences.append(edit_s)\r\n","  concat.append(s + '. ' + edit_s)\r\n","  fromto.append('From ' + word + ' to ' + edit)\r\n","\r\n","test_df['original_word'] = words \r\n","test_df['original'] = sentences\r\n","test_df['edit_sentence'] = edited_sentences\r\n","test_df['concat'] = concat\r\n","test_df['FromTo'] = fromto"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YgTa594WikoT"},"source":["# Number of epochs\r\n","epochs = 5\r\n","\r\n","# Proportion of training data for train compared to dev\r\n","train_proportion = 0.8\r\n","\r\n","def train(train_iter, dev_iter, model, number_epoch):\r\n","    \"\"\"\r\n","    Training loop for the model, which calls on eval to evaluate after each epoch\r\n","    \"\"\"\r\n","\r\n","    \r\n","    print(\"Training model.\")\r\n","    \r\n","    for epoch in range(1, number_epoch+1):\r\n","\r\n","        model.train()\r\n","        epoch_loss = 0\r\n","        epoch_sse = 0\r\n","        no_observations = 0  # Observations used for training so far\r\n","        \r\n","        y_pred = np.array([])\r\n","        y_true = np.array([])\r\n","\r\n","        for batch in train_iter:\r\n","            \r\n","            feature, target = batch\r\n","\r\n","            feature, target = feature.to(device), target.to(device)\r\n","\r\n","            # for RNN:\r\n","            model.batch_size = target.shape[0]\r\n","            no_observations = no_observations + target.shape[0]\r\n","            model.hidden = model.init_hidden()\r\n","\r\n","            predictions = model(feature).squeeze(1)\r\n","\r\n","            optimizer.zero_grad()\r\n","\r\n","            loss = loss_fn(predictions, target)\r\n","\r\n","            sse, __ = model_performance(predictions.detach().cpu().numpy(), target.detach().cpu().numpy())\r\n","\r\n","            loss.backward()\r\n","            optimizer.step()\r\n","\r\n","            epoch_loss += loss.item()*target.shape[0]\r\n","            epoch_sse += sse\r\n","\r\n","        valid_loss, valid_mse, __, __ = eval(dev_iter, model)\r\n","\r\n","        epoch_loss, epoch_mse = epoch_loss / no_observations, epoch_sse / no_observations\r\n","        print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.2f} | Train MSE: {epoch_mse:.2f} | Train RMSE: {epoch_mse**0.5:.2f} | \\\r\n","        Val. Loss: {valid_loss:.2f} | Val. MSE: {valid_mse:.2f} |  Val. RMSE: {valid_mse**0.5:.2f} |')\r\n","\r\n","def eval(data_iter, model):\r\n","    \"\"\"\r\n","    Evaluating model performance on the dev set\r\n","    \"\"\"\r\n","    model.eval()\r\n","    epoch_loss = 0\r\n","    epoch_sse = 0\r\n","    pred_all = []\r\n","    trg_all = []\r\n","    no_observations = 0\r\n","\r\n","    with torch.no_grad():\r\n","        for batch in data_iter:\r\n","            feature, target = batch\r\n","\r\n","            feature, target = feature.to(device), target.to(device)\r\n","\r\n","            # for RNN:\r\n","            model.batch_size = target.shape[0]\r\n","            no_observations = no_observations + target.shape[0]\r\n","            model.hidden = model.init_hidden()\r\n","\r\n","            predictions = model(feature).squeeze(1)\r\n","            loss = loss_fn(predictions, target)\r\n","\r\n","            # We get the mse\r\n","            pred, trg = predictions.detach().cpu().numpy(), target.detach().cpu().numpy()\r\n","            sse, __ = model_performance(pred, trg)\r\n","\r\n","            epoch_loss += loss.item()*target.shape[0]\r\n","            epoch_sse += sse\r\n","            pred_all.extend(pred)\r\n","            trg_all.extend(trg)\r\n","\r\n","    return epoch_loss/no_observations, epoch_sse/no_observations, np.array(pred_all), np.array(trg_all)\r\n","\r\n","def model_performance(output, target, print_output=False):\r\n","    \"\"\"\r\n","    Returns SSE and MSE per batch (printing the MSE and the RMSE)\r\n","    \"\"\"\r\n","\r\n","    sq_error = (output - target)**2\r\n","\r\n","    sse = np.sum(sq_error)\r\n","    mse = np.mean(sq_error)\r\n","    rmse = np.sqrt(mse)\r\n","\r\n","    if print_output:\r\n","        print(f'| MSE: {mse:.2f} | RMSE: {rmse:.2f} |')\r\n","\r\n","    return sse, mse\r\n","def create_vocab(data):\r\n","    \"\"\"\r\n","    Creating a corpus of all the tokens used\r\n","    \"\"\"\r\n","    tokenized_corpus = [] # Let us put the tokenized corpus in a list\r\n","\r\n","    for sentence in data:\r\n","\r\n","        tokenized_sentence = []\r\n","\r\n","        for token in sentence.split(' '): # simplest split is\r\n","\r\n","            tokenized_sentence.append(token)\r\n","\r\n","        tokenized_corpus.append(tokenized_sentence)\r\n","\r\n","    # Create single list of all vocabulary\r\n","    vocabulary = []  # Let us put all the tokens (mostly words) appearing in the vocabulary in a list\r\n","\r\n","    for sentence in tokenized_corpus:\r\n","\r\n","        for token in sentence:\r\n","\r\n","            if token not in vocabulary:\r\n","\r\n","                if True:\r\n","                    vocabulary.append(token)\r\n","\r\n","    return vocabulary, tokenized_corpus\r\n","\r\n","def collate_fn_padd(batch):\r\n","    '''\r\n","    We add padding to our minibatches and create tensors for our model\r\n","    '''\r\n","\r\n","    batch_labels = [l for f, l in batch]\r\n","    batch_features = [f for f, l in batch]\r\n","\r\n","    batch_features_len = [len(f) for f, l in batch]\r\n","\r\n","    seq_tensor = torch.zeros((len(batch), max(batch_features_len))).long()\r\n","\r\n","    for idx, (seq, seqlen) in enumerate(zip(batch_features, batch_features_len)):\r\n","        seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\r\n","\r\n","    batch_labels = torch.FloatTensor(batch_labels)\r\n","\r\n","    return seq_tensor, batch_labels\r\n","\r\n","class Task1Dataset(Dataset):\r\n","\r\n","    def __init__(self, train_data, labels):\r\n","        self.x_train = train_data\r\n","        self.y_train = labels\r\n","\r\n","    def __len__(self):\r\n","        return len(self.y_train)\r\n","\r\n","    def __getitem__(self, item):\r\n","        return self.x_train[item], self.y_train[item]\r\n","\r\n","class BiLSTM(nn.Module):\r\n","\r\n","    def __init__(self, embedding_dim, hidden_dim, vocab_size, batch_size, device):\r\n","        super(BiLSTM, self).__init__()\r\n","        self.hidden_dim = hidden_dim\r\n","        self.embedding_dim = embedding_dim\r\n","        self.device = device\r\n","        self.batch_size = batch_size\r\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\r\n","\r\n","        # The LSTM takes word embeddings as inputs, and outputs hidden states\r\n","        # with dimensionality hidden_dim.\r\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\r\n","\r\n","        # The linear layer that maps from hidden state space to tag space\r\n","        self.hidden2label = nn.Linear(hidden_dim * 2, 1)\r\n","        self.hidden = self.init_hidden()\r\n","\r\n","    def init_hidden(self):\r\n","        # Before we've done anything, we dont have any hidden state.\r\n","        # Refer to the Pytorch documentation to see exactly why they have this dimensionality.\r\n","        # The axes semantics are (num_layers * num_directions, minibatch_size, hidden_dim)\r\n","        return torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device), \\\r\n","               torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device)\r\n","\r\n","    def forward(self, sentence):\r\n","        try:\r\n","          embedded = self.embedding(sentence)\r\n","        except:\r\n","          print(\"PROBLEM\")\r\n","          print(sentence.shape)\r\n","          print(sentence)\r\n","        embedded = embedded.permute(1, 0, 2)\r\n","\r\n","        lstm_out, self.hidden = self.lstm(\r\n","            embedded.view(len(embedded), self.batch_size, self.embedding_dim), self.hidden)\r\n","\r\n","        out = self.hidden2label(lstm_out[-1])\r\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wNvWFPzQjtMO","executionInfo":{"elapsed":783563,"status":"ok","timestamp":1614636416141,"user":{"displayName":"Joël Tang","photoUrl":"","userId":"16257048276826027552"},"user_tz":0},"outputId":"8271458d-2ee0-4e0c-8b45-28861867bb44"},"source":["training_data = train_df['FromTo']\r\n","test_data = test_df['FromTo']\r\n","\r\n","# Creating word vectors\r\n","training_vocab, training_tokenized_corpus = create_vocab(training_data)\r\n","test_vocab, test_tokenized_corpus = create_vocab(test_data)\r\n","joint_vocab, joint_tokenized_corpus = create_vocab(pd.concat([training_data, test_data]))\r\n","print(\"Vocab created.\")\r\n","\r\n","wvecs = [] # word vectors\r\n","word2idx = [] # word2index\r\n","idx2word = []\r\n","\r\n","EMBEDDING_DIM = 100\r\n","embed_glove = 'glove.6B.{}d.txt'.format(EMBEDDING_DIM)\r\n","\r\n","# Add word 0 to vocab to keep good indexes\r\n","word2idx.append(('<pad>',0))\r\n","idx2word.append((0,'<pad>'))\r\n","wvecs.append(np.zeros(EMBEDDING_DIM))\r\n","\r\n","with codecs.open(embed_glove, 'r','utf-8') as f:\r\n","  index = 1\r\n","  for line in f.readlines():\r\n","    # Ignore the first line - first line typically contains vocab, dimensionality\r\n","    if len(line.strip().split()) > 3:\r\n","      word = line.strip().split()[0]\r\n","      if word in joint_vocab:\r\n","          (word, vec) = (word,\r\n","                     list(map(float,line.strip().split()[1:])))\r\n","          wvecs.append(vec)\r\n","          word2idx.append((word, index))\r\n","          idx2word.append((index, word))\r\n","          index += 1\r\n","\r\n","wvecs = np.array(wvecs)\r\n","word2idx = dict(word2idx)\r\n","idx2word = dict(idx2word)\r\n","\r\n","vectorized_seqs = [[word2idx[tok] for tok in seq if tok in word2idx] for seq in training_tokenized_corpus]\r\n","\r\n","# To avoid any sentences being empty (if no words match to our word embeddings)\r\n","vectorized_seqs = [x if len(x) > 0 else [0] for x in vectorized_seqs]\r\n","\r\n","INPUT_DIM = len(word2idx)\r\n","BATCH_SIZE = 32\r\n","\r\n","model = BiLSTM(EMBEDDING_DIM, 50, INPUT_DIM, BATCH_SIZE, device)\r\n","print(\"Model initialised.\")\r\n","\r\n","model.to(device)\r\n","# We provide the model with our embeddings\r\n","model.embedding.weight.data.copy_(torch.from_numpy(wvecs))\r\n","\r\n","feature = vectorized_seqs\r\n","\r\n","# 'feature' is a list of lists, each containing embedding IDs for word tokens\r\n","train_and_dev = Task1Dataset(feature, train_df['meanGrade'])\r\n","\r\n","train_examples = round(len(train_and_dev)*train_proportion)\r\n","dev_examples = len(train_and_dev) - train_examples\r\n","\r\n","train_dataset, dev_dataset = random_split(train_and_dev,\r\n","                                           (train_examples,\r\n","                                            dev_examples))\r\n","\r\n","train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\r\n","dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\r\n","\r\n","print(\"Dataloaders created.\")\r\n","\r\n","loss_fn = nn.MSELoss()\r\n","loss_fn = loss_fn.to(device)\r\n","\r\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\r\n","\r\n","epochs = 10\r\n","train(train_loader, dev_loader, model, epochs)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Vocab created.\n","Model initialised.\n","Dataloaders created.\n","Training model.\n","| Epoch: 01 | Train Loss: 0.78 | Train MSE: 0.78 | Train RMSE: 0.88 |         Val. Loss: 0.37 | Val. MSE: 0.37 |  Val. RMSE: 0.61 |\n","| Epoch: 02 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |         Val. Loss: 0.33 | Val. MSE: 0.33 |  Val. RMSE: 0.57 |\n","| Epoch: 03 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |         Val. Loss: 0.32 | Val. MSE: 0.32 |  Val. RMSE: 0.56 |\n","| Epoch: 04 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.56 |\n","| Epoch: 05 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.56 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.56 |\n","| Epoch: 06 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.56 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.55 |\n","| Epoch: 07 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.55 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.55 |\n","| Epoch: 08 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.55 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.55 |\n","| Epoch: 09 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.54 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.55 |\n","| Epoch: 10 | Train Loss: 0.29 | Train MSE: 0.29 | Train RMSE: 0.54 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.55 |\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UYfiIUm6jwKH"},"source":["##BERT Transformer"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lrF9g-50jz3Q","executionInfo":{"elapsed":789180,"status":"ok","timestamp":1614636421763,"user":{"displayName":"Joël Tang","photoUrl":"","userId":"16257048276826027552"},"user_tz":0},"outputId":"cf8b737d-392e-48dc-c0b2-fbc17fc2a9ed"},"source":["!pip install transformers==3.0.0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers==3.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/35/1c3f6e62d81f5f0daff1384e6d5e6c5758682a8357ebc765ece2b9def62b/transformers-3.0.0-py3-none-any.whl (754kB)\n","\u001b[K     |████████████████████████████████| 757kB 16.6MB/s \n","\u001b[?25hCollecting tokenizers==0.8.0-rc4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/82/0e82a95bd9db2b32569500cc1bb47aa7c4e0f57aa5e35cceba414096917b/tokenizers-0.8.0rc4-cp37-cp37m-manylinux1_x86_64.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 39.9MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (20.9)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (2.23.0)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n","\u001b[K     |████████████████████████████████| 1.2MB 55.3MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 47.9MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (1.19.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (4.41.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (2019.12.20)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.0) (2.4.7)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (1.0.1)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=fd371f05a7d1e3e7e9f72d701b41663a0e7580bf9ad1c6e0bc47eec707de90d6\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.95 tokenizers-0.8.0rc4 transformers-3.0.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"p5yYBqicM3Ho"},"source":["Loading of train and val set, tokenization of our dataset with pretrained BERT tokenizer"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["c79feb70451d4149adcb316bdea40d5e","15b52452fd2247ed875f835417e0f3a7","aa1cea8cbc484bd1aa3acdb5e50c492d","d10ab4d0d1264104b96b65f9dfdf63a7","8b95e769e29343ad8c9e41f9b18f872b","e32657f20b0645af91dc6c8984581603","adef4d20afa84b11811a8e04fccd86a5","0927c68261ea4d6eab5e9c944843bb8c"]},"id":"WEnuUvPuj1lH","executionInfo":{"elapsed":796228,"status":"ok","timestamp":1614636428812,"user":{"displayName":"Joël Tang","photoUrl":"","userId":"16257048276826027552"},"user_tz":0},"outputId":"a4358be5-d67c-41ba-ed15-38fca20a52d1"},"source":["from transformers import BertTokenizer\r\n","import torch\r\n","from sklearn.model_selection import train_test_split\r\n","\r\n","train, val = train_test_split(train_df, test_size=0.2)\r\n","\r\n","# Load the BERT tokenizer.\r\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\",do_lower_case=True)\r\n","\r\n","# Get the lists of sentences and their labels.\r\n","sentences = train['edit_sentence'].values\r\n","labels = train['meanGrade'].values\r\n","\r\n","# Tokenize all of the sentences and map the tokens to thier word IDs.\r\n","\r\n","input_ids = []\r\n","attention_masks = []\r\n","\r\n","input_ids_val = []\r\n","attention_masks_val = []\r\n","\r\n","for sent in sentences:\r\n","    encoded_dict = tokenizer.encode_plus(\r\n","                        sent,                      # Sentence to encode.\r\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\r\n","                        max_length = 32,           # Pad & truncate all sentences.\r\n","                        pad_to_max_length = True,\r\n","                        return_attention_mask = True,   # Construct attn. masks.\r\n","                        return_tensors = 'pt',     # Return pytorch tensors.\r\n","                        truncation = True\r\n","                   )\r\n","    \r\n","    # Add the encoded sentence to the list.    \r\n","    input_ids.append(encoded_dict['input_ids'])\r\n","    \r\n","    # And its attention mask (simply differentiates padding from non-padding).\r\n","    attention_masks.append(encoded_dict['attention_mask'])\r\n","\r\n","# Convert the lists into tensors.\r\n","input_ids = torch.cat(input_ids, dim=0)\r\n","attention_masks = torch.cat(attention_masks, dim=0)\r\n","labels = torch.tensor(labels)\r\n","\r\n","sentences_val = val['edit_sentence'].values\r\n","labels_val = val['meanGrade'].values\r\n","\r\n","input_ids_val = []\r\n","attention_masks_val = []\r\n","\r\n","for sent in sentences_val:\r\n","    encoded_dict = tokenizer.encode_plus(\r\n","                        sent,                      # Sentence to encode.\r\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\r\n","                        max_length = 32,           # Pad & truncate all sentences.\r\n","                        pad_to_max_length = True,\r\n","                        return_attention_mask = True,   # Construct attn. masks.\r\n","                        return_tensors = 'pt',     # Return pytorch tensors.\r\n","                        truncation = True\r\n","                   )\r\n","    input_ids_val.append(encoded_dict['input_ids'])    \r\n","    attention_masks_val.append(encoded_dict['attention_mask'])\r\n","\r\n","# Convert the lists into tensors.\r\n","input_ids_val = torch.cat(input_ids_val, dim=0)\r\n","attention_masks_val = torch.cat(attention_masks_val, dim=0)\r\n","labels_val = torch.tensor(labels_val)\r\n","\r\n","sentences_test = test_df['edit_sentence'].values\r\n","\r\n","input_ids_test = []\r\n","attention_masks_test = []\r\n","\r\n","for sent in sentences_test:\r\n","    encoded_dict = tokenizer.encode_plus(\r\n","                        sent,                      # Sentence to encode.\r\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\r\n","                        max_length = 32,           # Pad & truncate all sentences.\r\n","                        pad_to_max_length = True,\r\n","                        return_attention_mask = True,   # Construct attn. masks.\r\n","                        return_tensors = 'pt',     # Return pytorch tensors.\r\n","                        truncation = True\r\n","                   )\r\n","    input_ids_test.append(encoded_dict['input_ids'])    \r\n","    attention_masks_test.append(encoded_dict['attention_mask'])\r\n","\r\n","input_ids_test = torch.cat(input_ids_test, dim=0)\r\n","attention_masks_test = torch.cat(attention_masks_test, dim=0)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c79feb70451d4149adcb316bdea40d5e","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"d6JeKeqENMKE"},"source":["Creation of our TensorDataset and DataLoaders"]},{"cell_type":"code","metadata":{"id":"lI1Khuq4j7Y2"},"source":["from torch.utils.data import TensorDataset\r\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\r\n","\r\n","train_dataset = TensorDataset(input_ids, attention_masks, labels)\r\n","val_dataset = TensorDataset(input_ids_val, attention_masks_val, labels_val)\r\n","dev_dataset = TensorDataset(input_ids_test, attention_masks_test)\r\n","batch_size = 32\r\n","\r\n","train_dataloader = DataLoader(\r\n","            train_dataset,  # The training samples.\r\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\r\n","            batch_size = batch_size # Trains with this batch size.\r\n","        )\r\n","\r\n","validation_dataloader = DataLoader(\r\n","            val_dataset, # The validation samples.\r\n","            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\r\n","            batch_size = batch_size # Evaluate with this batch size.\r\n","        )\r\n","\r\n","dev_dataloader = DataLoader(\r\n","            dev_dataset, # The validation samples.\r\n","            sampler = SequentialSampler(dev_dataset), # Pull out batches sequentially.\r\n","            batch_size = len(dev_dataset) # Evaluate with this batch size.\r\n","        )\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"96L68z46NTQq"},"source":["Initialization of our BertForSequenceClassification model(regression because num_labels = 1)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":240,"referenced_widgets":["ac7ffcd28287428fb5ec4ba5e8033ef0","0731254f538846a9bb6cea3c61bb75b0","9579d4fdeb9c402ebed1dc910f0f3dd0","a0471dc7c7b545599cccbc16ffe56c8a","f01f590b37d948ac80f4e66bc2df9c61","73415ffff2884c85b4d0ff8a9be0edf9","b269fb9e00a14ebdba2fbee13899c1ea","65fc402c6cd24c33b1a46b0d54a66a36","fde43c77e82a47d1b34c84d8119339c6","75880863fd9f49adbb45dcdfd7229c43","c70bd1684de94e53bef2b06b8aa3b698","cae50ab3ef1c4950a9c89e0d1e692343","216900567c904412a0ef94c00bea05b6","e6fdbda3df5f4198914f4c8c9e6bb8e5","d9e5e204ccc140d5bf22b4ac00e481ed","802c8fb1a4564b269649443708912632"]},"id":"UBluI2ghkCW3","executionInfo":{"elapsed":810492,"status":"ok","timestamp":1614636443083,"user":{"displayName":"Joël Tang","photoUrl":"","userId":"16257048276826027552"},"user_tz":0},"outputId":"9f6253f0-4c6a-4acd-af67-44df2453de0b"},"source":["from transformers import get_linear_schedule_with_warmup\r\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\r\n","import random\r\n"," \r\n","model = BertForSequenceClassification.from_pretrained(\r\n","    'bert-base-uncased', # Use the 12-layer BERT model, with an uncased vocab.\r\n","    num_labels = 1,  \r\n","    output_attentions = False, # Whether the model returns attentions weights.\r\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\r\n",")\r\n","model.cuda()\r\n","model = model.double()\r\n","optimizer = AdamW(model.parameters(),\r\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5,\r\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\r\n","                )\r\n","\r\n","epochs = 3\r\n","total_steps = len(train_dataloader) * epochs\r\n","scheduler = get_linear_schedule_with_warmup(optimizer, \r\n","                                            num_warmup_steps = 0, # Default value in run_glue.py\r\n","                                            num_training_steps = total_steps)\r\n","GPU = True # Choose whether to use GPU\r\n","if GPU:\r\n","    device = torch.device(\"cuda\"  if torch.cuda.is_available() else \"cpu\")\r\n","else:\r\n","    device = torch.device(\"cpu\")\r\n","print(f'Using {device}')\r\n","\r\n","seed_val = 42\r\n","random.seed(seed_val)\r\n","np.random.seed(seed_val)\r\n","torch.manual_seed(seed_val)\r\n","torch.cuda.manual_seed_all(seed_val)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ac7ffcd28287428fb5ec4ba5e8033ef0","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fde43c77e82a47d1b34c84d8119339c6","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["Using cuda\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RkdfBNiVNjNL"},"source":["Custom train and eval functions for BERT model"]},{"cell_type":"code","metadata":{"id":"r0ruFcgRkFsL"},"source":["def eval_BERT(data_iter, model):\r\n","    print(\"Running Validation...\")\r\n","    model.eval()\r\n","\r\n","    # Tracking variables \r\n","    total_eval_accuracy = 0\r\n","    total_eval_loss = 0\r\n","    nb_eval_steps = 0\r\n","\r\n","    y_pred = np.array([])\r\n","    y_true = np.array([])\r\n","\r\n","    # Evaluate data for one epoch\r\n","    for batch in data_iter:\r\n","        \r\n","        b_input_ids = batch[0].to(device)\r\n","        b_input_mask = batch[1].to(device)\r\n","        b_labels = batch[2].to(device)\r\n","        \r\n","        with torch.no_grad():        \r\n","            (loss, logits) = model(b_input_ids, \r\n","                                   token_type_ids=None, \r\n","                                   attention_mask=b_input_mask,\r\n","                                   labels=b_labels)\r\n"," \r\n","        total_eval_loss += loss.item()\r\n","\r\n","        # Move logits and labels to CPU\r\n","        logits = logits.detach().cpu().numpy()\r\n","        label_ids = b_labels.to('cpu').numpy()\r\n","        y_pred = np.append(y_pred,logits)\r\n","        y_true = np.append(y_true,label_ids)\r\n","    sse, mse = model_performance(y_pred, y_true)\r\n","\r\n","    avg_val_loss = total_eval_loss / len(validation_dataloader)\r\n","    return avg_val_loss, mse, y_pred, y_true\r\n","\r\n","\r\n","def train_BERT(train_iter, dev_iter, model, number_epoch):\r\n","    for epoch_i in range(1, number_epoch+1):        \r\n","\r\n","        total_train_loss = 0\r\n","        train_mse = 0\r\n","        model.train()\r\n","        y_pred = np.array([])\r\n","        y_true = np.array([])\r\n","\r\n","        for step, batch in enumerate(train_iter):\r\n","            # if step % 40 == 0 and not step == 0:\r\n","            #     print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_iter)))\r\n","\r\n","            b_input_ids = batch[0].to(device)\r\n","            b_input_mask = batch[1].to(device)\r\n","            b_labels = batch[2].to(device)\r\n","\r\n","            model.zero_grad()        \r\n","\r\n","            loss, logits = model(b_input_ids, \r\n","                                token_type_ids=None, \r\n","                                attention_mask=b_input_mask, \r\n","                                labels=b_labels)\r\n","            \r\n","            logits = logits.detach().cpu().numpy()\r\n","            label_ids = b_labels.to('cpu').numpy()\r\n","            y_pred = np.append(y_pred,logits)\r\n","            y_true = np.append(y_true,label_ids) \r\n","\r\n","            total_train_loss += loss.item()\r\n","\r\n","            loss.backward()\r\n","\r\n","            # Clip the norm of the gradients to 1.0.\r\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n","\r\n","            optimizer.step()\r\n","            scheduler.step()\r\n","        _, train_mse = model_performance(y_pred, y_true) \r\n","        avg_train_loss = total_train_loss / len(train_iter) \r\n","        valid_loss, valid_mse, __, __ = eval_BERT(dev_iter, model)\r\n","        print(f'| Epoch: {epoch_i:02} | Train Loss: {avg_train_loss:.2f} | Train MSE: {train_mse:.2f} | Train RMSE: {train_mse**0.5:.2f} | \\\r\n","        Val. Loss: {valid_loss:.2f} | Val. MSE: {valid_mse:.2f} |  Val. RMSE: {valid_mse**0.5:.2f} |')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lQaSIimkNm5p"},"source":["Training of BERT model"]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"s8z7WqAZkJCj","outputId":"c67f73fd-7f8a-4be9-e34a-fc711e023db2"},"source":["train_BERT(train_dataloader, validation_dataloader, model, epochs)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Running Validation...\n","| Epoch: 01 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |         Val. Loss: 0.29 | Val. MSE: 0.29 |  Val. RMSE: 0.54 |\n","Running Validation...\n","| Epoch: 02 | Train Loss: 0.29 | Train MSE: 0.29 | Train RMSE: 0.53 |         Val. Loss: 0.31 | Val. MSE: 0.31 |  Val. RMSE: 0.55 |\n","Running Validation...\n","| Epoch: 03 | Train Loss: 0.23 | Train MSE: 0.23 | Train RMSE: 0.48 |         Val. Loss: 0.30 | Val. MSE: 0.31 |  Val. RMSE: 0.55 |\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VoFB4udYLZmL"},"source":["# **Approach 2** \n","\n","Bellow, no embedded representations or pre trained models are used.\n","\n","This approach is based on the assumption that the funny aspect of the data set relies on how absurd it is to find the edited word in the provided context.\n","The chosen inputs are the full original headlines (without tokens) and the edited version of the headlines. After some pre-processing, we compute a TF-IDF representation of the inputs and perform dimensionality reduction with PCA. Finally, a gradient based regressor, namely XGboost regressor, is tuned and trained to predict the grades."]},{"cell_type":"markdown","metadata":{"id":"ldzyQbZBKW99"},"source":["## Set up\n"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"ao6gIqXmKJQy","outputId":"b91a9756-252d-444b-8b29-e2f72c5d85a5"},"source":["# Imports\n","\n","import torch\n","import torch.nn as nn\n","import pandas as pd\n","import numpy as np\n","from sklearn.feature_extraction.text import CountVectorizer\n","from torch.utils.data import Dataset, random_split\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","import codecs\n","\n","import nltk\n","from nltk.stem import WordNetLemmatizer \n","nltk.download('wordnet')\n","nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"h9mHs1loKSJD","outputId":"e78f68ea-d9a5-413b-b92b-7babc243976c"},"source":["# Setting random seed and device\n","SEED = 2\n","\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n","print(device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"s1PRi7ByKduZ"},"source":["## Reload data\r\n","\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"fqtDk8keFOoi"},"source":["Here, the data is read and the distribution of the mean grade is plotted, we can see that most headlines have grades lower than 2."]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"RLiGohQOFNK0","outputId":"d8d75add-8fbb-48b1-9392-1983be13e75d"},"source":["# Read data\r\n","train_df = pd.read_csv(path + 'train.csv')\r\n","test_df = pd.read_csv(path + 'dev.csv')\r\n","\r\n","# Plot the data distribution\r\n","import matplotlib.pyplot as plt\r\n","plt.hist(train_df['meanGrade'], bins= 32, cumulative=False)\r\n","plt.title(\"meanGrade distribtuion\")\r\n","plt.show()\r\n","plt.hist(train_df['meanGrade'], bins= 64, cumulative=True)\r\n","plt.title(\"Cumulative meanGrade distribtuion\")\r\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"b7eskzr1Pqms"},"source":["## Pre-processing"]},{"cell_type":"markdown","metadata":{"id":"dDrqkQNlFjMb"},"source":["We remove the tags from the headlines and create different dictionnaries. Each one encapsulates a version of the data: the edited work, the original sentence without the word to edit, the original word that was used to be edited etc.\r\n","Even though all of them are not used in this notebook, we compared the performance of the regression obtained from different combinations of them and kept the one that led to the lowest RMSE. This combination is made of the original sentence (with the tag of the word to edit removed) and the edited verrsion of the sentence."]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"BKV54aNVPuwT","outputId":"f3ab5fde-27d6-4c1c-ab8c-463cfaceb579"},"source":["\n","\n","###### identify and remove tags for the word to edit ######\n","##### create different dictionaries depending on the information we keep ######\n","def remove_edit_tags(corpus):  \n","  words = []\n","  full_sentences = []\n","  context_sentences = []\n","  edited_sentences = []\n","  for i, s in enumerate(corpus['original'].tolist()):\n","    start = s.find('<') + 1\n","    end = s.find('/>', start)\n","    words.append(s[start:end])\n","    s = s.replace('<','')\n","    s = s.replace('/>','')  \n","    full_sentences.append(s)\n","    context_sentences.append(s[:start-1]+s[end:])\n","    edited_sentences.append(s[:start-1]+corpus['edit'][i] + \" \" + s[end:])\n","  corpus['original_word'] = words\n","  corpus['original'] = full_sentences\n","  corpus['context'] = context_sentences\n","  corpus['edited_sentences'] = edited_sentences\n","  return(corpus)\n","\n","print(\"** Original sentence: **\")\n","print(train_df['original'][1])\n","\n","train_no_edited_tags = remove_edit_tags(train_df)\n","test_no_edited_tags = remove_edit_tags(test_df)\n","\n","print(\"\\n** Example of a pair of sentences after tag removal: **\")\n","print(train_no_edited_tags['original'][1])\n","print(train_no_edited_tags['edited_sentences'][1])\n","\n","\n","\n","\n","####### Split for trainig and valdiation 80/20 and creates features\n","indices = np.arange(0, len(train_df)) \n","np.random.shuffle(indices)\n","train_idx = indices[:int(0.8*len(train_df))]\n","val_idx = indices[int(0.8*len(train_df)):] \n","\n","# training set with original and edited sentences\n","x_train = [train_df['original'][train_idx].tolist(), train_df['edited_sentences'][train_idx].tolist()]\n","y_train = train_df['meanGrade'][train_idx].tolist()\n","y_train = np.array(y_train)\n","\n","# validation set with original and edited sentences\n","x_val = [train_df['original'][val_idx].tolist(), train_df['edited_sentences'][val_idx].tolist()]\n","y_val = train_df['meanGrade'][val_idx].tolist()\n","y_val = np.array(y_val)\n","\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["** Original sentence: **\n","Pentagon claims 2,000 % increase in Russian trolls after <Syria/> strikes . What does that mean ?\n","\n","** Example of a pair of sentences after tag removal: **\n","Pentagon claims 2,000 % increase in Russian trolls after Syria strikes . What does that mean ?\n","Pentagon claims 2,000 % increase in Russian trolls after bowling strikes . What does that mean ?\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LvCYIllkK4T_"},"source":["In the cell bellow, the sentences are processed before being transformed in the TF IDF representation.\r\n","\r\n","Because no embedded representation is used, we work on the full sentences to keep as much information as possible. However, stop words and regexp allows us to get rid of words or symbols that do not carry a relevant meaning. The data set being small, lemmatization helps the model to identify similar words and group them together in the counts. All sentences become lower-case in order to have a consistant format. Not all headlines of the corpus are capitalized. \r\n"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"xjPY4ogz_Ps5","outputId":"b5125f67-108c-4e8a-ebde-d67eb9bf4ffb"},"source":["############# TF IDF #############\n","\n","import re\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","lemmatizer = WordNetLemmatizer()\n","stopword = stopwords.words('english')\n","stopword += ['s', \"'\"]\n","\n","\n","\n","\n","\n","# tokenizer + lemmatization + lowercase + stop words + number removal\n","def home_tokenizer(sentence):\n","  t = []\n","  # Tokenize + lowercase\n","  tokens = word_tokenize(sentence.lower())\n","  # Stop words + Lemmatization\n","  for word in tokens:\n","    # regexp to remove numbers\n","    word = re.sub(r\"\\d+[.,]*[0-9]+\", \"\", word)\n","    if word not in stopword and len(word)>1:\n","      t.append(lemmatizer.lemmatize(word))\n","  return t\n","\n","\n","print(\"\\n\\nExample of the home_tokenizer function:\")\n","print(\"original\",word_tokenize(x_train[0][0]))\n","print(\"home_tokenizer\",home_tokenizer(x_train[0][0]))\n","\n","\n","\n","\n","##### Train Tf-idf model #####\n","\n","count_vect = CountVectorizer(stop_words=None, tokenizer=home_tokenizer, ngram_range=(1,1)) \n","\n","# fit original to Tf-idff \n","train_original_counts = count_vect.fit_transform(x_train[0])\n","transformer = TfidfTransformer().fit(train_original_counts)\n","train_original_counts = transformer.transform(train_original_counts).toarray()\n","\n","# transform edited sentences to tf-idf \n","train_edit_counts = count_vect.transform(x_train[1])\n","train_edit_counts = transformer.transform(train_edit_counts).toarray()\n","\n","\n","\n","\n","##### Validation Tf-idf model #####\n","\n","# fit original to Tf-idff,  on all the training set\n","train_and_val_original_counts = count_vect.transform(train_df['original'])\n","transformer = TfidfTransformer().fit(train_and_val_original_counts)\n","val_original_counts = count_vect.transform(x_val[0])\n","val_original_counts = transformer.transform(val_original_counts).toarray()\n","\n","# # transform edited sentences to tf-idf\n","val_edit_counts = count_vect.transform(x_val[1])\n","val_edit_counts = transformer.transform(val_edit_counts).toarray()\n","\n","\n","\n","##### Test Tf-idf model #####\n","\n","# fit original to Tf-idff,  on all the data set (train and test)\n","test_original_counts = count_vect.transform( pd.concat([train_df['original'], test_df['original']]))\n","transformer = TfidfTransformer().fit(test_original_counts)\n","test_original_counts = count_vect.transform(test_df['original'])\n","test_original_counts = transformer.transform(test_original_counts).toarray()\n","\n","# transform edited sentences to tf-idf\n","test_edit_counts = count_vect.transform(test_df['edited_sentences'])\n","test_edit_counts = transformer.transform(test_edit_counts).toarray()\n","\n","\n","\n","\n","\n","##### create final train and validation sets that can be fed to the regressor#####\n","\n","# final training set\n","x_final_train = np.concatenate( (train_original_counts, train_edit_counts), axis=1 )\n","\n","# final validation set\n","x_final_val = np.concatenate( (val_original_counts, val_edit_counts), axis=1 )\n","\n","\n","##### create final test set to be fed to the regressor#####\n","x_final_test = np.concatenate( (test_original_counts, test_edit_counts), axis=1 )\n","\n","print(x_final_train.shape)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","\n","\n","Example of the home_tokenizer function:\n","original ['Trump', 'Administration', 'Rolls', 'Back', 'Rules', 'Protecting', 'Transgender', 'Inmates']\n","home_tokenizer ['trump', 'administration', 'roll', 'back', 'rule', 'protecting', 'transgender', 'inmate']\n","(7721, 12914)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uHNOlzanKu3R"},"source":["## Model\n"]},{"cell_type":"markdown","metadata":{"id":"smRzgjDHZHUu"},"source":["Baseline of the model"]},{"cell_type":"markdown","metadata":{"id":"5Nq-jAWGQFk-"},"source":["Baseline\n","\n","| MSE: 0.34 | RMSE: 0.58 |"]},{"cell_type":"markdown","metadata":{"id":"GeB9noc8K801"},"source":["## Evaluate performance"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"yoNU7LlIW784"},"source":["# How we print the model performance\n","def model_performance(output, target, print_output=False):\n","    \"\"\"\n","    Returns SSE and MSE per batch (printing the MSE and the RMSE)\n","    \"\"\"\n","\n","    sq_error = (output - target)**2\n","\n","    sse = np.sum(sq_error)\n","    mse = np.mean(sq_error)\n","    rmse = np.sqrt(mse)\n","\n","    if print_output:\n","        print(f'| MSE: {mse:.2f} | RMSE: {rmse:.2f} |')\n","\n","    return sse, mse"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XUpS36QoVd7R"},"source":["The features of our final vectors are the concatenation of TF-IDF representation of two sentences that are very close to each other. PCA is performed to alleviate redundancy while keeping 99% of explained variance. Also,dimensionaility reduction makes the regressor work faster."]},{"cell_type":"code","metadata":{"id":"5DR_vF2lznpN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614638960488,"user_tz":0,"elapsed":276366,"user":{"displayName":"Joël Tang","photoUrl":"","userId":"16257048276826027552"}},"outputId":"2e915092-f109-44f3-9337-b506b520b57a"},"source":["from sklearn.decomposition import PCA\r\n","from sklearn.preprocessing import StandardScaler\r\n","pca = PCA(n_components = 0.99)\r\n","scaler = StandardScaler()\r\n","\r\n","# Normalize the data and fit PCA\r\n","scaler.fit(x_final_train)\r\n","x_final_train = scaler.transform(x_final_train)\r\n","pca.fit(x_final_train)\r\n","Xtrain = pca.transform(x_final_train)\r\n","\r\n","x_final_val = scaler.transform(x_final_val)\r\n","Xval = pca.transform(x_final_val)\r\n","\r\n","print(\"Shape before PCA :\")\r\n","print(\"x_final_train shape: \", x_final_train.shape)\r\n","print(\"x_final_train val: \", x_final_val.shape)\r\n","\r\n","print(\"\\n\\nShape after PCA :\")\r\n","print(\"Xtrain shape: \", Xtrain.shape)\r\n","print(\"Xval shape: \", Xval.shape)\r\n"],"execution_count":30,"outputs":[{"output_type":"stream","text":["Shape before PCA :\n","x_final_train shape:  (7721, 12914)\n","x_final_train val:  (1931, 12914)\n","\n","\n","Shape after PCA :\n","Xtrain shape:  (7721, 3994)\n","Xval shape:  (1931, 3994)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KU-y4izQX5HZ"},"source":["Use XGboost regressor with tuned parameters."]},{"cell_type":"code","metadata":{"id":"8hYYhH8rLEQl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614639304187,"user_tz":0,"elapsed":343712,"user":{"displayName":"Joël Tang","photoUrl":"","userId":"16257048276826027552"}},"outputId":"5148c4fc-9f80-4a7d-bcd0-117e8102ea27"},"source":["# test\n","import xgboost as xg \n","\n","eval_set = [(Xtrain, y_train), (Xval, y_val)]\n","\n","# model\n","xgb_r = xg.XGBRegressor(n_estimators=300, verbosity=1, max_depth=3, gamma = 0.01,\n","                        colsample_bytree=1, reg_alpha=0.1, reg_lambda = 1,\n","                        learning_rate = 0.05) \n"," \n","# Fitting the model \n","print(Xtrain.shape)\n","xgb_r.fit(Xtrain, y_train, eval_set= eval_set, early_stopping_rounds=30) \n","  \n","# Predict the model \n","pred_train = xgb_r.predict(Xtrain) \n","pred_val = xgb_r.predict(Xval)\n","\n","\n"],"execution_count":31,"outputs":[{"output_type":"stream","text":["(7721, 3994)\n","[22:49:20] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n","[0]\tvalidation_0-rmse:0.718072\tvalidation_1-rmse:0.701599\n","Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n","\n","Will train until validation_1-rmse hasn't improved in 30 rounds.\n","[1]\tvalidation_0-rmse:0.705623\tvalidation_1-rmse:0.689667\n","[2]\tvalidation_0-rmse:0.694096\tvalidation_1-rmse:0.678576\n","[3]\tvalidation_0-rmse:0.683533\tvalidation_1-rmse:0.668503\n","[4]\tvalidation_0-rmse:0.673798\tvalidation_1-rmse:0.659053\n","[5]\tvalidation_0-rmse:0.664837\tvalidation_1-rmse:0.650493\n","[6]\tvalidation_0-rmse:0.656625\tvalidation_1-rmse:0.642719\n","[7]\tvalidation_0-rmse:0.649082\tvalidation_1-rmse:0.635508\n","[8]\tvalidation_0-rmse:0.642142\tvalidation_1-rmse:0.629004\n","[9]\tvalidation_0-rmse:0.635723\tvalidation_1-rmse:0.623359\n","[10]\tvalidation_0-rmse:0.62986\tvalidation_1-rmse:0.617767\n","[11]\tvalidation_0-rmse:0.624462\tvalidation_1-rmse:0.612592\n","[12]\tvalidation_0-rmse:0.619507\tvalidation_1-rmse:0.608206\n","[13]\tvalidation_0-rmse:0.614942\tvalidation_1-rmse:0.604336\n","[14]\tvalidation_0-rmse:0.610726\tvalidation_1-rmse:0.600729\n","[15]\tvalidation_0-rmse:0.606906\tvalidation_1-rmse:0.59737\n","[16]\tvalidation_0-rmse:0.603459\tvalidation_1-rmse:0.594328\n","[17]\tvalidation_0-rmse:0.600185\tvalidation_1-rmse:0.591447\n","[18]\tvalidation_0-rmse:0.597145\tvalidation_1-rmse:0.588972\n","[19]\tvalidation_0-rmse:0.594327\tvalidation_1-rmse:0.586622\n","[20]\tvalidation_0-rmse:0.591694\tvalidation_1-rmse:0.584537\n","[21]\tvalidation_0-rmse:0.589247\tvalidation_1-rmse:0.582424\n","[22]\tvalidation_0-rmse:0.586995\tvalidation_1-rmse:0.580803\n","[23]\tvalidation_0-rmse:0.585065\tvalidation_1-rmse:0.579368\n","[24]\tvalidation_0-rmse:0.583252\tvalidation_1-rmse:0.578\n","[25]\tvalidation_0-rmse:0.581458\tvalidation_1-rmse:0.576856\n","[26]\tvalidation_0-rmse:0.579761\tvalidation_1-rmse:0.57564\n","[27]\tvalidation_0-rmse:0.578191\tvalidation_1-rmse:0.574633\n","[28]\tvalidation_0-rmse:0.576691\tvalidation_1-rmse:0.573621\n","[29]\tvalidation_0-rmse:0.575298\tvalidation_1-rmse:0.5727\n","[30]\tvalidation_0-rmse:0.57412\tvalidation_1-rmse:0.572131\n","[31]\tvalidation_0-rmse:0.572883\tvalidation_1-rmse:0.571584\n","[32]\tvalidation_0-rmse:0.571749\tvalidation_1-rmse:0.570897\n","[33]\tvalidation_0-rmse:0.570681\tvalidation_1-rmse:0.570358\n","[34]\tvalidation_0-rmse:0.569678\tvalidation_1-rmse:0.569779\n","[35]\tvalidation_0-rmse:0.568844\tvalidation_1-rmse:0.569381\n","[36]\tvalidation_0-rmse:0.567852\tvalidation_1-rmse:0.568839\n","[37]\tvalidation_0-rmse:0.566943\tvalidation_1-rmse:0.568164\n","[38]\tvalidation_0-rmse:0.566268\tvalidation_1-rmse:0.567866\n","[39]\tvalidation_0-rmse:0.565493\tvalidation_1-rmse:0.567725\n","[40]\tvalidation_0-rmse:0.564746\tvalidation_1-rmse:0.567433\n","[41]\tvalidation_0-rmse:0.564042\tvalidation_1-rmse:0.567173\n","[42]\tvalidation_0-rmse:0.563498\tvalidation_1-rmse:0.567095\n","[43]\tvalidation_0-rmse:0.562829\tvalidation_1-rmse:0.566862\n","[44]\tvalidation_0-rmse:0.562133\tvalidation_1-rmse:0.566566\n","[45]\tvalidation_0-rmse:0.561506\tvalidation_1-rmse:0.566312\n","[46]\tvalidation_0-rmse:0.560726\tvalidation_1-rmse:0.56624\n","[47]\tvalidation_0-rmse:0.560028\tvalidation_1-rmse:0.566222\n","[48]\tvalidation_0-rmse:0.559426\tvalidation_1-rmse:0.566066\n","[49]\tvalidation_0-rmse:0.558888\tvalidation_1-rmse:0.565958\n","[50]\tvalidation_0-rmse:0.558281\tvalidation_1-rmse:0.565866\n","[51]\tvalidation_0-rmse:0.557685\tvalidation_1-rmse:0.565658\n","[52]\tvalidation_0-rmse:0.557221\tvalidation_1-rmse:0.565538\n","[53]\tvalidation_0-rmse:0.556742\tvalidation_1-rmse:0.565516\n","[54]\tvalidation_0-rmse:0.556139\tvalidation_1-rmse:0.565481\n","[55]\tvalidation_0-rmse:0.555533\tvalidation_1-rmse:0.565375\n","[56]\tvalidation_0-rmse:0.555084\tvalidation_1-rmse:0.565264\n","[57]\tvalidation_0-rmse:0.554507\tvalidation_1-rmse:0.565327\n","[58]\tvalidation_0-rmse:0.554017\tvalidation_1-rmse:0.565218\n","[59]\tvalidation_0-rmse:0.553652\tvalidation_1-rmse:0.565121\n","[60]\tvalidation_0-rmse:0.553132\tvalidation_1-rmse:0.565017\n","[61]\tvalidation_0-rmse:0.552825\tvalidation_1-rmse:0.565049\n","[62]\tvalidation_0-rmse:0.55229\tvalidation_1-rmse:0.565078\n","[63]\tvalidation_0-rmse:0.551968\tvalidation_1-rmse:0.565006\n","[64]\tvalidation_0-rmse:0.55141\tvalidation_1-rmse:0.564873\n","[65]\tvalidation_0-rmse:0.550878\tvalidation_1-rmse:0.564912\n","[66]\tvalidation_0-rmse:0.550304\tvalidation_1-rmse:0.564694\n","[67]\tvalidation_0-rmse:0.549869\tvalidation_1-rmse:0.564527\n","[68]\tvalidation_0-rmse:0.549299\tvalidation_1-rmse:0.564518\n","[69]\tvalidation_0-rmse:0.548925\tvalidation_1-rmse:0.564284\n","[70]\tvalidation_0-rmse:0.548675\tvalidation_1-rmse:0.564293\n","[71]\tvalidation_0-rmse:0.548168\tvalidation_1-rmse:0.564154\n","[72]\tvalidation_0-rmse:0.547659\tvalidation_1-rmse:0.564193\n","[73]\tvalidation_0-rmse:0.54715\tvalidation_1-rmse:0.564259\n","[74]\tvalidation_0-rmse:0.546689\tvalidation_1-rmse:0.564286\n","[75]\tvalidation_0-rmse:0.546335\tvalidation_1-rmse:0.564247\n","[76]\tvalidation_0-rmse:0.545835\tvalidation_1-rmse:0.564168\n","[77]\tvalidation_0-rmse:0.545474\tvalidation_1-rmse:0.564248\n","[78]\tvalidation_0-rmse:0.544942\tvalidation_1-rmse:0.56414\n","[79]\tvalidation_0-rmse:0.544423\tvalidation_1-rmse:0.563925\n","[80]\tvalidation_0-rmse:0.543915\tvalidation_1-rmse:0.563818\n","[81]\tvalidation_0-rmse:0.543608\tvalidation_1-rmse:0.563728\n","[82]\tvalidation_0-rmse:0.543123\tvalidation_1-rmse:0.563599\n","[83]\tvalidation_0-rmse:0.542769\tvalidation_1-rmse:0.563733\n","[84]\tvalidation_0-rmse:0.542202\tvalidation_1-rmse:0.563633\n","[85]\tvalidation_0-rmse:0.541915\tvalidation_1-rmse:0.563666\n","[86]\tvalidation_0-rmse:0.541596\tvalidation_1-rmse:0.563574\n","[87]\tvalidation_0-rmse:0.541261\tvalidation_1-rmse:0.563564\n","[88]\tvalidation_0-rmse:0.540773\tvalidation_1-rmse:0.563637\n","[89]\tvalidation_0-rmse:0.540301\tvalidation_1-rmse:0.563586\n","[90]\tvalidation_0-rmse:0.539887\tvalidation_1-rmse:0.563575\n","[91]\tvalidation_0-rmse:0.539461\tvalidation_1-rmse:0.563495\n","[92]\tvalidation_0-rmse:0.538969\tvalidation_1-rmse:0.563418\n","[93]\tvalidation_0-rmse:0.538705\tvalidation_1-rmse:0.563462\n","[94]\tvalidation_0-rmse:0.538366\tvalidation_1-rmse:0.563515\n","[95]\tvalidation_0-rmse:0.537918\tvalidation_1-rmse:0.563341\n","[96]\tvalidation_0-rmse:0.537464\tvalidation_1-rmse:0.563297\n","[97]\tvalidation_0-rmse:0.53722\tvalidation_1-rmse:0.563224\n","[98]\tvalidation_0-rmse:0.536757\tvalidation_1-rmse:0.5634\n","[99]\tvalidation_0-rmse:0.536148\tvalidation_1-rmse:0.563395\n","[100]\tvalidation_0-rmse:0.535844\tvalidation_1-rmse:0.563359\n","[101]\tvalidation_0-rmse:0.535353\tvalidation_1-rmse:0.563427\n","[102]\tvalidation_0-rmse:0.534866\tvalidation_1-rmse:0.563372\n","[103]\tvalidation_0-rmse:0.534625\tvalidation_1-rmse:0.563388\n","[104]\tvalidation_0-rmse:0.534166\tvalidation_1-rmse:0.563382\n","[105]\tvalidation_0-rmse:0.533662\tvalidation_1-rmse:0.563344\n","[106]\tvalidation_0-rmse:0.533359\tvalidation_1-rmse:0.563246\n","[107]\tvalidation_0-rmse:0.532927\tvalidation_1-rmse:0.562986\n","[108]\tvalidation_0-rmse:0.532437\tvalidation_1-rmse:0.562917\n","[109]\tvalidation_0-rmse:0.532024\tvalidation_1-rmse:0.562878\n","[110]\tvalidation_0-rmse:0.531717\tvalidation_1-rmse:0.562948\n","[111]\tvalidation_0-rmse:0.531237\tvalidation_1-rmse:0.563075\n","[112]\tvalidation_0-rmse:0.530778\tvalidation_1-rmse:0.563102\n","[113]\tvalidation_0-rmse:0.53029\tvalidation_1-rmse:0.562962\n","[114]\tvalidation_0-rmse:0.529943\tvalidation_1-rmse:0.563003\n","[115]\tvalidation_0-rmse:0.529487\tvalidation_1-rmse:0.562841\n","[116]\tvalidation_0-rmse:0.529021\tvalidation_1-rmse:0.562823\n","[117]\tvalidation_0-rmse:0.52857\tvalidation_1-rmse:0.562842\n","[118]\tvalidation_0-rmse:0.528184\tvalidation_1-rmse:0.562892\n","[119]\tvalidation_0-rmse:0.527723\tvalidation_1-rmse:0.562851\n","[120]\tvalidation_0-rmse:0.527398\tvalidation_1-rmse:0.562722\n","[121]\tvalidation_0-rmse:0.527065\tvalidation_1-rmse:0.562794\n","[122]\tvalidation_0-rmse:0.526637\tvalidation_1-rmse:0.56269\n","[123]\tvalidation_0-rmse:0.526267\tvalidation_1-rmse:0.56291\n","[124]\tvalidation_0-rmse:0.525964\tvalidation_1-rmse:0.562878\n","[125]\tvalidation_0-rmse:0.525532\tvalidation_1-rmse:0.562953\n","[126]\tvalidation_0-rmse:0.525098\tvalidation_1-rmse:0.562918\n","[127]\tvalidation_0-rmse:0.52477\tvalidation_1-rmse:0.562908\n","[128]\tvalidation_0-rmse:0.524549\tvalidation_1-rmse:0.563042\n","[129]\tvalidation_0-rmse:0.524113\tvalidation_1-rmse:0.563118\n","[130]\tvalidation_0-rmse:0.523681\tvalidation_1-rmse:0.563179\n","[131]\tvalidation_0-rmse:0.523379\tvalidation_1-rmse:0.563053\n","[132]\tvalidation_0-rmse:0.522922\tvalidation_1-rmse:0.563079\n","[133]\tvalidation_0-rmse:0.522403\tvalidation_1-rmse:0.563036\n","[134]\tvalidation_0-rmse:0.521944\tvalidation_1-rmse:0.563013\n","[135]\tvalidation_0-rmse:0.521421\tvalidation_1-rmse:0.563051\n","[136]\tvalidation_0-rmse:0.521036\tvalidation_1-rmse:0.562988\n","[137]\tvalidation_0-rmse:0.520682\tvalidation_1-rmse:0.562859\n","[138]\tvalidation_0-rmse:0.520391\tvalidation_1-rmse:0.562757\n","[139]\tvalidation_0-rmse:0.520116\tvalidation_1-rmse:0.562748\n","[140]\tvalidation_0-rmse:0.519688\tvalidation_1-rmse:0.56264\n","[141]\tvalidation_0-rmse:0.519439\tvalidation_1-rmse:0.562532\n","[142]\tvalidation_0-rmse:0.519018\tvalidation_1-rmse:0.562534\n","[143]\tvalidation_0-rmse:0.518755\tvalidation_1-rmse:0.562516\n","[144]\tvalidation_0-rmse:0.518485\tvalidation_1-rmse:0.562525\n","[145]\tvalidation_0-rmse:0.518231\tvalidation_1-rmse:0.562549\n","[146]\tvalidation_0-rmse:0.517888\tvalidation_1-rmse:0.562609\n","[147]\tvalidation_0-rmse:0.517588\tvalidation_1-rmse:0.562457\n","[148]\tvalidation_0-rmse:0.517279\tvalidation_1-rmse:0.562472\n","[149]\tvalidation_0-rmse:0.516797\tvalidation_1-rmse:0.562308\n","[150]\tvalidation_0-rmse:0.516351\tvalidation_1-rmse:0.562285\n","[151]\tvalidation_0-rmse:0.516094\tvalidation_1-rmse:0.562305\n","[152]\tvalidation_0-rmse:0.515733\tvalidation_1-rmse:0.562264\n","[153]\tvalidation_0-rmse:0.515241\tvalidation_1-rmse:0.562165\n","[154]\tvalidation_0-rmse:0.514823\tvalidation_1-rmse:0.562134\n","[155]\tvalidation_0-rmse:0.514457\tvalidation_1-rmse:0.562164\n","[156]\tvalidation_0-rmse:0.514228\tvalidation_1-rmse:0.5622\n","[157]\tvalidation_0-rmse:0.513885\tvalidation_1-rmse:0.562209\n","[158]\tvalidation_0-rmse:0.513505\tvalidation_1-rmse:0.562178\n","[159]\tvalidation_0-rmse:0.513256\tvalidation_1-rmse:0.562198\n","[160]\tvalidation_0-rmse:0.51284\tvalidation_1-rmse:0.562083\n","[161]\tvalidation_0-rmse:0.51241\tvalidation_1-rmse:0.562124\n","[162]\tvalidation_0-rmse:0.512131\tvalidation_1-rmse:0.562042\n","[163]\tvalidation_0-rmse:0.511851\tvalidation_1-rmse:0.562099\n","[164]\tvalidation_0-rmse:0.511442\tvalidation_1-rmse:0.562101\n","[165]\tvalidation_0-rmse:0.51103\tvalidation_1-rmse:0.562054\n","[166]\tvalidation_0-rmse:0.510716\tvalidation_1-rmse:0.562001\n","[167]\tvalidation_0-rmse:0.510429\tvalidation_1-rmse:0.562015\n","[168]\tvalidation_0-rmse:0.509976\tvalidation_1-rmse:0.562131\n","[169]\tvalidation_0-rmse:0.509769\tvalidation_1-rmse:0.562207\n","[170]\tvalidation_0-rmse:0.509312\tvalidation_1-rmse:0.562095\n","[171]\tvalidation_0-rmse:0.50895\tvalidation_1-rmse:0.56204\n","[172]\tvalidation_0-rmse:0.508544\tvalidation_1-rmse:0.562156\n","[173]\tvalidation_0-rmse:0.508375\tvalidation_1-rmse:0.562237\n","[174]\tvalidation_0-rmse:0.507953\tvalidation_1-rmse:0.56219\n","[175]\tvalidation_0-rmse:0.507608\tvalidation_1-rmse:0.562245\n","[176]\tvalidation_0-rmse:0.507381\tvalidation_1-rmse:0.562212\n","[177]\tvalidation_0-rmse:0.506992\tvalidation_1-rmse:0.562142\n","[178]\tvalidation_0-rmse:0.506765\tvalidation_1-rmse:0.562091\n","[179]\tvalidation_0-rmse:0.506573\tvalidation_1-rmse:0.562073\n","[180]\tvalidation_0-rmse:0.506306\tvalidation_1-rmse:0.562089\n","[181]\tvalidation_0-rmse:0.506097\tvalidation_1-rmse:0.562147\n","[182]\tvalidation_0-rmse:0.505791\tvalidation_1-rmse:0.562171\n","[183]\tvalidation_0-rmse:0.505412\tvalidation_1-rmse:0.562288\n","[184]\tvalidation_0-rmse:0.505003\tvalidation_1-rmse:0.562271\n","[185]\tvalidation_0-rmse:0.504565\tvalidation_1-rmse:0.562247\n","[186]\tvalidation_0-rmse:0.504292\tvalidation_1-rmse:0.562211\n","[187]\tvalidation_0-rmse:0.503889\tvalidation_1-rmse:0.562175\n","[188]\tvalidation_0-rmse:0.503467\tvalidation_1-rmse:0.562253\n","[189]\tvalidation_0-rmse:0.503009\tvalidation_1-rmse:0.562242\n","[190]\tvalidation_0-rmse:0.50266\tvalidation_1-rmse:0.56219\n","[191]\tvalidation_0-rmse:0.502299\tvalidation_1-rmse:0.562292\n","[192]\tvalidation_0-rmse:0.502019\tvalidation_1-rmse:0.56237\n","[193]\tvalidation_0-rmse:0.501561\tvalidation_1-rmse:0.562344\n","[194]\tvalidation_0-rmse:0.501317\tvalidation_1-rmse:0.562323\n","[195]\tvalidation_0-rmse:0.501041\tvalidation_1-rmse:0.562199\n","[196]\tvalidation_0-rmse:0.500769\tvalidation_1-rmse:0.562208\n","Stopping. Best iteration:\n","[166]\tvalidation_0-rmse:0.510716\tvalidation_1-rmse:0.562001\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Vat06lTkYPKa"},"source":["Plot the learning curves"]},{"cell_type":"code","metadata":{"id":"9KeVNKfTT79R","colab":{"base_uri":"https://localhost:8080/","height":385},"executionInfo":{"status":"ok","timestamp":1614639304560,"user_tz":0,"elapsed":381,"user":{"displayName":"Joël Tang","photoUrl":"","userId":"16257048276826027552"}},"outputId":"2dc30ce8-7446-4cec-c35d-bd1914ddfd7f"},"source":["# We run the evaluation:\r\n","print(\"\\nTrain performance:\")\r\n","sse, mse = model_performance(pred_train, y_train, True)\r\n","\r\n","print(\"\\nDev performance:\")\r\n","sse, mse = model_performance(pred_val, y_val, True)\r\n","\r\n","\r\n","# retrieve performance metrics\r\n","results = xgb_r.evals_result()\r\n","epochs = len(results['validation_0']['rmse'])\r\n","x_axis = range(0, epochs)\r\n","# plot log loss\r\n","plt.plot(x_axis, results['validation_0']['rmse'], color = 'b', label='Train')\r\n","plt.plot(x_axis, results['validation_1']['rmse'], color = 'r', label='Validation')\r\n","plt.legend()\r\n","plt.ylabel('RMSE')\r\n","plt.title('Loss curve')\r\n","plt.show()\r\n"],"execution_count":32,"outputs":[{"output_type":"stream","text":["\n","Train performance:\n","| MSE: 0.26 | RMSE: 0.51 |\n","\n","Dev performance:\n","| MSE: 0.32 | RMSE: 0.56 |\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]}]}